---
title: NeuralNetConfiguration类
layout: cn-default
---
<h1>NeuralNetConfiguration类</h1>
<p><em>DL4J神经网络构建器基础</em></p>
<p>DL4J中的所有神经网络均以NeuralNetConfiguration构造器为基础创建。该对象极其灵活，无论您需要实现什么类型的神经网络层，几乎都可以通过它来构建。这个类的参数组合及配置可用于设定不同类型的层，包括受限玻尔兹曼机（RBM）、深度置信网络（DBN）、卷积神经网络（CNN）、自动编码器等。各项参数及其默认设置的说明如下：</p>
<p>开始构建单个层的Java类的方法：</p>
<pre><code class="language-java">
NeuralNetConfiguration conf = new NeuralNetConfiguration.Builder()
</code></pre>
<br>
<p>您可以用以下方式为这个类追加参数：</p>
<pre><code class="language-java">
new NeuralNetConfiguration.Builder().iterations(100).layer(new RBM()).nIn(784).nOut(10)
</code></pre>
<p>参数：</p>
<ul>
  <li><strong>activationFunction</strong>：<em>string</em>，每个隐藏层节点的激活函数，
    <ul>
      <li>默认 = &ldquo;sigmoid&rdquo;</li>
      <li>选项：
        <ul>
          <li>&ldquo;abs&rdquo;</li>
          <li>&ldquo;acos&rdquo;</li>
          <li>&ldquo;asin&rdquo;</li>
          <li>&ldquo;atan&rdquo;</li>
          <li>&ldquo;ceil&rdquo;</li>
          <li>&ldquo;cos&rdquo;</li>
          <li>&ldquo;exp&rdquo;</li>
          <li>&ldquo;floor&rdquo;</li>
          <li>&ldquo;hardtanh&rdquo;</li>
          <li>&ldquo;identity&rdquo;</li>
          <li>&ldquo;maxout&rdquo;</li>
          <li>&ldquo;negative&rdquo;</li>
          <li>&ldquo;pow&rdquo;</li>
          <li>&ldquo;relu&rdquo;</li>
          <li>&ldquo;round&rdquo;</li>
          <li>&ldquo;sigmoid&rdquo;</li>
          <li>&ldquo;sign&rdquo;</li>
          <li>&ldquo;sin&rdquo;</li>
          <li>&ldquo;softmax&rdquo;</li>
          <li>&ldquo;sqrt&rdquo;</li>
          <li>&ldquo;stabilize&rdquo;</li>
          <li>&ldquo;tahn&rdquo;</li>
          <li>用nd4j.getExecutioner创建自定义函数</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>applySparsity</strong>：<em>boolean</em>，有活动的二进制隐藏单元时使用
    <ul>
      <li>默认 = false</li>
    </ul>
  </li>
  <li><strong>batch</strong>：<em>int</em>，输入神经网络的数据量
    <ul>
      <li>默认 = 0</li>
    </ul>
  </li>
  <li><strong>constrainGradientToUnitNorm</strong>：<em>boolean</em>，帮助梯度收敛，让损失变得更小、更平滑（避免梯度膨胀）
    <ul>
      <li>默认 = false</li>
    </ul>
  </li>
  <li><strong>convolutionType</strong>：<em>ConvolutionLayer.ConvolutionType class</em>，卷积层类型
    <ul>
      <li>默认 = ConvolutionLayer.ConvolutionType.MAX</li>
    </ul>
  </li>
  <li><strong>corruptionLevel</strong>：<em>double</em>，对输入数据进行污染的程度
    <ul>
      <li>默认 = 0.3</li>
    </ul>
  </li>
  <li><strong>dist</strong>：<em>Distribution class</em>，权重初始化所用的分布
    <ul>
      <li>默认 = new NormalDistribution(1e-3,1)</li>
      <li>选项：
        <ul>
          <li>NormalDistribution</li>
          <li>UniformDistribution</li>
          <li>BinomialDistribution</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>dropOut</strong>：<em>double</em>，随机丢弃一定数量的活动单元/节点，将其置零（不激活）
    <ul>
      <li>默认 = 0</li>
    </ul>
  </li>
  <li><strong>featureMapSize</strong>：<em>int[]</em>，卷积内核大小（也称为接受场）
    <ul>
      <li>默认 = {2,2}</li>
    </ul>
  </li>
  <li><strong>filterSize</strong>：<em>int[]</em>，为降采样层创建张量数据结构 = 特征映射图数量（深度切片数量） x 通道数量 x 特征映射图空间（输入数据矩阵的行与列）
    <ul>
      <li>默认 = {2,2,2,2}</li>
      <li>例如：5, 1, numRows, numColumns</li>
      <li>行 = 批次或总体的数据样本量；列 = 每个数据样本的特征数量</li>
    </ul>
  </li>
  <li><strong>hiddenUnit</strong>：<em>RBM.HiddenUnit</em>，RBM隐藏单元/节点类型
    <ul>
      <li>默认 = RBM.HiddenUnit.BINARY</li>
    </ul>
  </li>
  <li><strong>inputPreProcessor</strong>：(<em>int</em>, <em>class</em>) {层数, 数据处理器类} 对层的输入数据进行转换/预处理
    <ul>
      <li>例如：.inputPreProcessor(0,new ConvolutionInputPreProcessor(numRows,numColumns))</li>
      <li>将2维张量转换为4维</li>
      <li>行 = 批次；列 = 输入数据点的数量</li>
    </ul>
  </li>
  <li><strong>iterations</strong>：<em>int</em>，定型迭代次数</li>
  <li><strong>k</strong>：<em>int</em>，RBM各层的预定型中，用马尔可夫链进行预测的对比散度算法的运行次数
    <ul>
      <li>默认 = 1</li>
    </ul>
  </li>
  <li><strong>kernel</strong>：<em>int[]</em>，内核大小（用于卷积网络）
    <ul>
      <li>默认 = 5</li>
    </ul>
  </li>
  <li><strong>l1</strong>：<em>double</em>，L1正则化
    <ul>
      <li>默认 = 0.0</li>
    </ul>
  </li>
  <li><strong>l2</strong>：<em>double</em>，L2正则化
    <ul>
      <li>默认 = 0.0</li>
    </ul>
  </li>
  <li><strong>layer</strong>：<em>Layer class</em>，设定层的结构</li>
  <li><strong>lossFunction</strong>：<em>LossFunctions class</em>，作用于网络输出的误差变换函数
    <ul>
      <li>默认 = LossFunctions.LossFunction.RECONSTRUCTION_CROSSENTROPY</li>
      <li>选项：
        <ul>
          <li>MSE</li>
          <li>EXPLL</li>
          <li>XENT</li>
          <li>MCXENT</li>
          <li>RMSE_XENT</li>
          <li>SQUARED_LOSS</li>
          <li>RECONSTRUCTION_CROSSENTROPY</li>
          <li>NEGATIVELOGLIKELIHOOD</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>learningRate</strong>：<em>Double</em>，步幅，亦即在搜索空间中移动时改变参数向量的速度（学习速率越大，得到最终结果的速度越快，但有可能错过最佳值；速率较小，所需的定型时间可能会大幅增加） 优化函数的变化速率
    <ul>
      <li>默认 = 1e-1f</li>
    </ul>
  </li>
  <li><strong>minimize</strong>：<em>boolean</em>，设定目标是最小化还是最大化
    <ul>
      <li>默认 = false</li>
    </ul>
  </li>
  <li><strong>momentum</strong>：<em>double</em>，动量，用于减少权重变化的波动
    <ul>
      <li>默认 = 0.5</li>
    </ul>
  </li>
  <li><strong>momentumAfter</strong>：<em>Map[Integer, Double]</em> （n次迭代，动量）n次迭代之后的动量</li>
  <li><strong>nIn</strong>：<em>int</em>，输入数据点的数量</li>
  <li><strong>nOut</strong>：<em>int</em>，输出节点数量</li>
  <li><strong>numIterations</strong>：<em>int</em>，网络定型的迭代次数
    <ul>
      <li>默认 = 1000</li>
    </ul>
  </li>
  <li><strong>numLineSearchIterations</strong>：<em>int</em>
    <ul>
      <li>默认 = 100</li>
    </ul>
  </li>
  <li><strong>optimizationAlgo</strong>：<em>OptimizationAlgorithm class</em>，反向传播算法
    <ul>
      <li>默认 = OptimizationAlgorithm.CONJUGATE_GRADIENT</li>
      <li>选项：
        <ul>
          <li>GRADIENT_DESCENT</li>
          <li>CONJUGATE_GRADIENT</li>
          <li>HESSIAN_FREE</li>
          <li>LBFGS</li>
          <li>ITERATION_GRADIENT_DESCENT</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>preProcessor</strong>：(<em>int</em>, <em>class</em>) {层数, 数据处理器类} 对层的输出数据进行转换/预处理
    <ul>
      <li>例如：.preProcessor(0, new ConvolutionPostProcessor())</li>
    </ul>
  </li>
  <li><strong>renderWeightsEveryNumEpochs</strong>：<em>int</em>，每过几个epoch显示权重，默认 = -1</li>
  <li><strong>resetAdaGradIterations</strong>：<em>int</em>，在n次迭代之后重置AdaGrad历史梯度
    <ul>
      <li>默认 = -1</li>
    </ul>
  </li>
  <li><strong>rng</strong>：<em>Random class</em>，用一个随机种子来确保定型的初始权重保持一致
    <ul>
      <li>默认 = new DefaultRandom()</li>
      <li>示例 = .rng(new DefaultRandom(3))</li>
    </ul>
  </li>
  <li><strong>stride</strong>：<em>int[]</em>，降采样类层的大小
    <ul>
      <li>默认 = {2,2}</li>
    </ul>
  </li>
  <li><strong>sparsity</strong>：<em>double</em>
    <ul>
      <li>默认 = 0</li>
    </ul>
  </li>
  <li><strong>stepFunction</strong>：<em>StepFunction class</em>，算法在学习过程中的权重调整幅度
    <ul>
      <li>默认 = new GradientStepFunction()</li>
    </ul>
  </li>
  <li><strong>useAdaGrad</strong>：<em>boolean</em>，在反向传播算法中采用AdaGrad学习速率适应
    <ul>
      <li>默认 = true</li>
    </ul>
  </li>
  <li><strong>useRegularization</strong>：<em>boolean</em>，采用正则化
    <ul>
      <li>默认 = false</li>
    </ul>
  </li>
  <li><strong>variables</strong>：<em>List[String]</em>，梯度的键，确保能够有序地获取和设定梯度
    <ul>
      <li>默认 = new ArrayList&lt;&gt;()</li>
    </ul>
  </li>
  <li><strong>visibleUnit</strong>：<em>RBM.VisibleUnit</em>，RBM可见单元/节点的类型，默认 = RBM.VisibleUnit.BINARY</li>
  <li><strong>weightInit</strong>：<em>WeightInit class</em>，权重初始化方式
    <ul>
      <li>默认 = WeightInit.VI</li>
      <li>选项：
        <ul>
          <li>WeightInit.DISTRIBUTION：用基于输入数据形状的分布来初始化权重</li>
          <li>WeightInit.NORMALIZED：用正态分布来初始化权重</li>
          <li>WeightInit.SIZE：用受限均匀分布来初始化权重，由数据形状决定最小值和最大值</li>
          <li>WeightInit.UNIFORM：用受限均匀分布来初始化权重（指定最小值和最大值）</li>
          <li>WeightInit.VI：用归一化方差来初始化权重（Glorot）</li>
          <li>WeightInit.ZERO：初始权重设为零</li>
        </ul>
      </li>
    </ul>
  </li>
  <li><strong>weightShape</strong>：<em>int[]</em></li>
</ul>
<p>该类的更多详情参见<a href="http://deeplearning4j.org/doc/">JavaDoc</a>。</p>
