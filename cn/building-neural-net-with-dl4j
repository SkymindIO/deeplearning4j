---
layout: cn-default
title: 用DeepLearning4J构建神经网络
---

# 用DeepLearning4J构建神经网络

目录

* [MultiLayerNetwork与ComputationGraph](#multilayer)
* [网络配置](#configuration)
* [配置细节](#configurationdetails)
* [为网络添加用户界面](#ui)


## <a name="multilayer">MultiLayerNetwork与ComputationGraph</a>

Deeplearning4J有两个用于构建和训练神经网络的类：MultiLayerNetwork（多层网络）与ComputationGraph（计算图）。 

### MultiLayerNetwork

MultiLayerNetwork是一种由多个层堆叠而成的神经网络，通常有一个输出层。 

MultiLayerNetwork可通过反向传播法进行训练，可选择进行预训练，具体取决于网络包含哪种类型的层。

### ComputationGraph

ComputationGraph是连接结构更为复杂的神经网络。ComputationGraph让层与层之间可以通过指定方向的有向无环图结构连接。 

用户还可以任意指定ComputationGraph的输入与输出数量。

## <a name="configuration">网络配置</a>

无论是ComputationGraph或MultiLayerNetwork，使用前都要先用Configuration类进行配置。 

两个配置类均提供方便易用的构建器架构。 

### ComputationGraph的配置

以下是一个循环神经网络的配置，取自我们的示例。 

```
ComputationGraphConfiguration configuration = new NeuralNetConfiguration.Builder()
   	.weightInit(WeightInit.XAVIER)
	.learningRate(0.5)
	.updater(Updater.RMSPROP)
	.optimizationAlgo(OptimizationAlgorithm.STOCHASTIC_GRADIENT_DESCENT).iterations(nIterations)
	.seed(seed)
	.graphBuilder()
	.addInputs("additionIn", "sumOut")
	.setInputTypes(InputType.recurrent(FEATURE_VEC_SIZE), InputType.recurrent(FEATURE_VEC_SIZE))
	.addLayer("encoder", new GravesLSTM.Builder().nIn(FEATURE_VEC_SIZE).nOut(numHiddenNodes).activation("softsign").build(),"additionIn")
	.addVertex("lastTimeStep", new LastTimeStepVertex("additionIn"), "encoder")
	.addVertex("duplicateTimeStep", new DuplicateToTimeSeriesVertex("sumOut"), "lastTimeStep")
	.addLayer("decoder", new GravesLSTM.Builder().nIn(FEATURE_VEC_SIZE+numHiddenNodes).nOut(numHiddenNodes).activation("softsign").build(), "sumOut","duplicateTimeStep")
	.addLayer("output", new RnnOutputLayer.Builder().nIn(numHiddenNodes).nOut(FEATURE_VEC_SIZE).activation("softmax").lossFunction(LossFunctions.LossFunction.MCXENT).build(), "decoder")
	.setOutputs("output")
	.pretrain(false).backprop(true)
	.build();
				
```				

### MultiLayerNetwork的配置

以下是一个简单的前馈网络的配置，同样来自我们的示例。 

```
MultiLayerConfiguration conf = new NeuralNetConfiguration.Builder()
	.seed(seed)
    .iterations(iterations)
	 .activation("tanh")
	 .weightInit(WeightInit.XAVIER)
	 .learningRate(0.1)
	 .regularization(true).l2(1e-4)
	 .list()
	 .layer(0, new DenseLayer.Builder().nIn(numInputs).nOut(3)
	     .build())
	 .layer(1, new DenseLayer.Builder().nIn(3).nOut(3)
	     .build())
	 .layer(2, new OutputLayer.Builder(LossFunctions.LossFunction.NEGATIVELOGLIKELIHOOD)
	     .activation("softmax")
	     .nIn(3).nOut(outputNum).build())
	 .backprop(true).pretrain(false)
	 .build();
			
			
```			


## <a name="configurationdetails">配置细节</a>

### 随机种子

两种类型的神经网络通常都要设定一个随机种子。将随机种子编入源码中，就能得到可重复的结果。随机种子用于神经网络权重的初始化。 

### 迭代

一次迭代就是神经网络模型参数的一次更新。 

注意迭代与epoch不同，后者指对数据集进行一次完整遍历。 

一个epoch中可能会进行多次迭代。如您仅在遍历整个数据集一次之后更新参数，epoch与迭代的意义是相同的；但假如您按微批次来进行更新，两者就有不同的含义。假设您的数据有两个微批次：A和B，则.numIterations(3)的设定会让网络以AAABBB的顺序进行训练，而3个epoch的训练顺序则是ABABAB。

### 激活函数


激活函数决定节点根据输入生成怎样的输出。Sigmoid激活函数过去一直很受欢迎，目前非常流行的则是ReLU函数。DeepLearnging4j中的激活函数以层为对象进行设定，应用于该层中的所有神经元。


配置激活函数

```
layer(2, new OutputLayer.Builder(LossFunctions.LossFunction.NEGATIVELOGLIKELIHOOD).activation(Activation.SOFTMAX)
``` 

### 输出层激活函数

网络输出取决于输出层的激活函数。输出层的激活函数则取决于网络所要回答的问题。

让我们举一些例子。 

若要生成Binary(0,1)的输出，请在输出层中使用sigmoid激活函数。输出结果会在0与1之间，可视为0或1的概率。 

若要生成类型目标输出（采用“1-of-C”式编码），请在输出层中使用softmax激活函数。输出节点数量应与类型数量相等。输出结果可视为每个标签的概率。 

若要生成有限范围的连续值输出，请使用sigmoid或tanh激活函数（输出范围缩放至目标范围）。

若要生成无已知上限的正输出，请使用ReLU类激活函数、softplus激活函数（或者用对数标准化转换为无限连续值）。

若要生成无限连续值输出，请使用线性激活函数（等于没有激活函数）。

### 支持的激活函数


DeepLearning4J支持下列激活函数

* CUBE
* ELU
* HARDSIGMOID
* HARDTANH
* IDENTITY
* LEAKYRELU
* RATIONALTANH
* RELU
* RRELU
* SIGMOID
* SOFTMAX
* SOFTPLUS
* SOFTSIGN
* TANH

### 自定义激活函数

DeepLearning4J支持自定义激活函数。 

### 权重初始化

神经网络的起始权重是随机设定的。随机化很重要，如果两个神经元的权重相同，它们就永远无法分化并习得不同的特征。Xavier初始化目前被广泛使用，我们在[术语表](https://deeplearning4j.org/cn/glossary.html#xavier)中对其作了介绍。

### 学习速率

学习速率决定权重更新时误差减少的推进幅度。学习速率过高，误差有可能大幅波动，无法收敛。学习速率过低，网络的训练时间可能变得过长。较高的学习速率有时会取得比较好的初始效果，但随着模型不断接受训练，往往还是需要调低学习速率。ADAM或AdaGrad等适应型更新器可以满足调整学习速率的需求。 


### 反向传播

更有效的神经网络训练方法会使用反向传播算法或其变体。 

### 预训练

在对受限玻尔兹曼机和自动编码器等特殊网络进行无监督训练时，将预训练设为true可能会有帮助。其他网络应设为false。 

### 正则化 

正则化是防止神经网络对训练数据过拟合的方法。过拟合指网络对训练数据的预测变得非常准确，但无法很好地推广至一般情况，处理测试数据时的性能表现较差。 

Deeplearning4J同时支持L1和L2正则化。  

### 添加层

若要为网络配置添加一个层，请对ComputationGraph使用addLayer，对MultiLayerNetworks使用.layer。每一层的激活函数分别设置。损失函数在输出层中设置。 


### 更新器

DL4J支持下列更新器

* ADADELTA
* ADAGRAD
* ADAM
* NESTEROVS
* NONE
* RMSPROP
* SGD
* CONJUGATE GRADIENT（共轭梯度法）
* HESSIAN FREE（无Hessian矩阵法）
* LBFGS
* LINE GRADIENT DESCENT（线搜索梯度下降法）

更新器的JavaDoc是DeepLearning4j JavaDoc的一部分，参见[此处](https://deeplearning4j.org/doc/org/deeplearning4j/nn/conf/Updater.html)。



### 更新器的动画 

感谢Alec Radford授权我们使用下列动画

#### 更新器在复杂的误差曲面上移动

![Alt text](../img/updater_animation.gif)
<br />
<br />

#### 更新器在复杂度较低的误差曲面上移动

![Alt text](../img/updater_animation2.gif)





### 侦听器 

您可以为网络添加收集统计数据或报告进度的侦听器。以下是为模型添加一个侦听器的代码示例，请注意，您可以用setListeners添加多个侦听器。 

```
model.setListeners(new ScoreIterationListener(100));
```

侦听器的应用实例之一是在网络训练过程中将准确率显示于控制台。 

另一个例子是为用户界面提供统计信息。 

```

UIServer uiServer = UIServer.getInstance();
        StatsStorage statsStorage = new InMemoryStatsStorage();
        model.setListeners(new StatsListener(statsStorage),new ScoreIterationListener(1));
        uiServer.attach(statsStorage);
```		


#### CollectScoresIterationListener	
CollectScoresIterationListener的功能就是将模型分值（与迭代一起）存储于内部，每1次或N次迭代（具体次数可设置）存储1次。

#### ComposableIterationListener	
一组侦听器

### ParamAndGradientIterationListener	
提供训练过程中每次迭代的参数及梯度等详细信息的迭代侦听器。

### PerformanceListener	
一种简单的迭代侦听器，用于记录每次迭代训练所用的时间。


## <a name="ui">为神经网络添加用户界面</a>

DL4J有一个基于网页的用户界面可供使用。运行相关Java代码的计算机可以通过端口9000访问服务器。为了使用界面，需要创建一个UIServer的实例。您可以为神经网络创建一个StatsStorage侦听器，然后将其添加至用户界面服务器。 

代码示例如下。 

```
UIServer uiServer = UIServer.getInstance();
StatsStorage statsStorage = new InMemoryStatsStorage();
model.setListeners(new StatsListener(statsStorage),new ScoreIterationListener(1));
uiServer.attach(statsStorage);

```
