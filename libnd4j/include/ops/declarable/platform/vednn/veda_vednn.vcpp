#include <stdio.h>
#include <veda_device.h>
#include <vednn.h>

#include <cmath>
#include <iostream>
#include <memory>
//#define SHOW_ON_FUNC_ENTRY 1
#if !defined(SHOW_ON_FUNC_ENTRY)
#define LOG_FUNC()
#else
#define LOG_FUNC() printf("%s in [%s %d]\n", __PRETTY_FUNCTION__, __FILE__, __LINE__);
#endif

inline void copyTo_nhwc_generic(const vednnTensorParam_t &p, const float *nchw, float *nhwc) {
  int hs = p.width;
  int cs = p.width * p.height;
  int ns = cs * p.channel;
  int ws2 = p.channel;
  int hs2 = ws2 * p.width;
#pragma omp parallel for
  for (int n = 0; n < p.batch; n++) {
    for (int h = 0; h < p.height; h++) {
      for (int w = 0; w < p.width; w++) {
        for (int c = 0; c < p.channel; c++) {
          nhwc[n * ns + h * hs2 + w * ws2 + c] = nchw[n * ns + h * hs + c * cs + w];
        }
      }
    }
  }
}

inline void copyTo_nchw_generic(const vednnTensorParam_t &p, const float *nhwc, float *nchw) {
  constexpr int cs = 1;
  int ws = p.channel;
  int hs = ws * p.width;
  int ns = hs * p.height;

  constexpr int ws1 = 1;
  int hs1 = p.width;
  int cs1 = hs1 * p.height;
  int ns1 = cs1 * p.channel;
#pragma omp parallel for
  for (int n = 0; n < p.batch; n++) {
    for (int h = 0; h < p.height; h++) {
      for (int w = 0; w < p.width; w++) {
        for (int c = 0; c < p.channel; c++) {
          nchw[n * ns1 + h * hs1 + w * ws1 + c * cs1] = nhwc[n * ns + h * hs + w * ws + c * cs];
        }
      }
    }
  }
}

void copyFromNCHW(const vednnTensorParam_t &param, const float *nchw_data, float *nhwc_data) {
  return copyTo_nhwc_generic(param, nchw_data, nhwc_data);
}

float *getNCHW(const vednnTensorParam_t &param, float *nhwc_data, std::unique_ptr<float[]> &temp) {
  if (param.channel == 1) {
    // there is not any need for conversion
    return nhwc_data;
  } else {
    int hwSize = param.height * param.width;
    int strideN = hwSize * param.channel;
    size_t length = param.batch * strideN;
    temp.reset(new float[length]);
    float *nchw_data = temp.get();
    copyTo_nchw_generic(param, nhwc_data, nchw_data);

    return nchw_data;
  }
}

void showBuffer(float *x, int l) {
  for (int i = 0; i < l; i++) std::cout << x[i] << ", ";
  std::cout << std::endl;
}

float *getWeightFormat1Data(const vednnFilterParam_t &paramFilter, float *weight, int wFormat,
                            std::unique_ptr<float[]> &temp) {
  // 0 - [kH, kW, iC, oC], 1 - [oC, iC, kH, kW], 2 - [oC, kH, kW, iC]
  if (wFormat == 1) {
    return weight;
  } else {
    if (wFormat == 2) {
      //[oC, kH, kW, iC] -> [oC, iC, kH, kW],
      vednnTensorParam_t param;
      param.dtype = DTYPE_FLOAT;
      param.batch = paramFilter.outChannel;
      param.channel = paramFilter.inChannel;
      param.height = paramFilter.height;
      param.width = paramFilter.width;
      auto w = getNCHW(param, weight, temp);
      return w;
    } else {
      //[kH, kW, iC, oC] -> [oC, iC, kH, kW]
      constexpr int ocs0 = 1;
      int ics0 = paramFilter.outChannel;
      int ws0 = ics0 * paramFilter.inChannel;
      int hs0 = ws0 * paramFilter.width;

      size_t length = hs0 * paramFilter.height;
      temp.reset(new float[length]);
      float *ret = temp.get();
      constexpr int ws1 = 1;
      int hs1 = paramFilter.width;
      int ics1 = hs1 * paramFilter.height;
      int ocs1 = ics1 * paramFilter.inChannel;
//       // special kernel 3x3
//       if (paramFilter.width == 3 && paramFilter.height == 3) {
// // std::cout<<"_---"<<std::endl;
//         auto weight_0 = weight;
//         auto weight_1 = weight + ws0;
//         auto weight_2 = weight + ws0 + ws0;

//         auto weight1_0 = weight_0 + hs0;
//         auto weight1_1 = weight_1 + hs0;
//         auto weight1_2 = weight_2 + hs0;

//         auto weight2_0 = weight1_0 + hs0;
//         auto weight2_1 = weight1_1 + hs0;
//         auto weight2_2 = weight1_2 + hs0;

//         auto ret_0 = ret;
//         auto ret_1 = ret + 1;
//         auto ret_2 = ret + 2;

//         auto ret1_0 = ret_0 + hs1;
//         auto ret1_1 = ret_1 + hs1;
//         auto ret1_2 = ret_2 + hs1;

//         auto ret2_0 = ret1_0 + hs1;
//         auto ret2_1 = ret1_1 + hs1;
//         auto ret2_2 = ret1_2 + hs1;
// // float
// #pragma omp parallel for
//         for (int j = 0; j < paramFilter.inChannel; j++) {
//           for (int i = 0; i < paramFilter.outChannel; i++) {
//             ret_0[i * ocs1 + j * ics1] = weight_0[i + j * ics0];
//             ret_1[i * ocs1 + j * ics1] = weight_1[i + j * ics0];
//             ret_2[i * ocs1 + j * ics1] = weight_2[i + j * ics0];
//             ret1_0[i * ocs1 + j * ics1] = weight1_0[i + j * ics0];
//             ret1_1[i * ocs1 + j * ics1] = weight1_1[i + j * ics0];
//             ret1_2[i * ocs1 + j * ics1] = weight1_2[i + j * ics0];
//             ret2_0[i * ocs1 + j * ics1] = weight2_0[i + j * ics0];
//             ret2_1[i * ocs1 + j * ics1] = weight2_1[i + j * ics0];
//             ret2_2[i * ocs1 + j * ics1] = weight2_2[i + j * ics0];
//           }
//         }

//       } else {
#pragma omp parallel for
        for (int h = 0; h < paramFilter.height; h++)
          for (int w = 0; w < paramFilter.width; w++)
            for (int i = 0; i < paramFilter.inChannel; i++)
              for (int j = 0; j < paramFilter.outChannel; j++) {
                ret[j * ocs1 + i * ics1 + w * ws1 + h * hs1] = weight[j * ocs0 + i * ics0 + w * ws0 + h * hs0];
              }
//      }
      return ret;
    }
  }
}

extern "C" {

vednnError_t vedaVednnConvolutionForwardAddBias(const vednnTensorParam_t *paramIn, VEDAdeviceptr vDataIn,
                                                bool isDataInNCHW, const vednnFilterParam_t *paramFilter,
                                                VEDAdeviceptr vDataKernel, int WieghtFormat,
                                                const vednnBiasParam_t *paramBias, VEDAdeviceptr vDataBias,
                                                const vednnTensorParam_t *paramOut, VEDAdeviceptr vDataOut,
                                                bool isDataOutNCHW, const vednnConvolutionParam_t *paramConv,
                                                vednnConvolutionAlgorithm_t algo) {
  LOG_FUNC();
  vednnError_t res;
  void *pDataIn, *pDataBias = nullptr, *pDataKernelPtr;
  void *pDataOutPtr, *pDataOut = nullptr;
  vedaMemPtr((void **)&pDataIn, vDataIn);
  vedaMemPtr((void **)&pDataKernelPtr, vDataKernel);
  vedaMemPtr((void **)&pDataBias, vDataBias);
  vedaMemPtr((void **)&pDataOutPtr, vDataOut);
  std::unique_ptr<float[]> tempIn, tempOut, tempW;
  // printf("%d %d %d\n", (int)isDataInNCHW, (int)isDataOutNCHW, (int)WieghtFormat);
  if (!isDataInNCHW) {
    pDataIn = getNCHW(*paramIn, (float *)pDataIn, tempIn);
  }
  if (!isDataOutNCHW) {
    tempOut.reset(new float[paramOut->batch * paramOut->channel * paramOut->height * paramOut->width]);
    pDataOut = tempOut.get();
  } else {
    pDataOut = pDataOutPtr;
  }
  auto pDataKernel = getWeightFormat1Data(*paramFilter, (float *)pDataKernelPtr, WieghtFormat, tempW);

  // printf("inside ve %p %p %p %p\n", pDataIn, pDataKernel, pDataBias, pDataOut);
  if (pDataBias) {
    res = vednnConvolutionForwardAddBias(paramIn, pDataIn, paramFilter, pDataKernel, paramBias, pDataBias, paramOut,
                                         pDataOut, paramConv, algo);
  } else {
    res = vednnConvolutionForward(paramIn, pDataIn, paramFilter, pDataKernel, paramOut, pDataOut, paramConv, algo);
  }

  if (pDataOut != pDataOutPtr) {
    copyFromNCHW(*paramOut, (const float *)pDataOut, (float *)pDataOutPtr);
  }
  return res;
}

vednnError_t vedaVednnConvolutionBackwardDataAndFilter(
    const vednnTensorParam_t *paramGradOut, VEDAdeviceptr vGradOutData, const vednnFilterParam_t *paramFilter,
    VEDAdeviceptr vWeightData, VEDAdeviceptr vGradWeightData, const vednnTensorParam_t *paramGradIn,
    VEDAdeviceptr vInData, VEDAdeviceptr vGradInData, const vednnConvolutionParam_t *paramConv,
    vednnConvolutionAlgorithm_t algo) {
  LOG_FUNC();
  void *gradOutData, *weightData, *gradWeightsData, *inData, *gradInData;
  vedaMemPtr((void **)&gradOutData, vGradOutData);
  vedaMemPtr((void **)&weightData, vWeightData);
  vedaMemPtr((void **)&gradWeightsData, vGradWeightData);
  vedaMemPtr((void **)&inData, vInData);
  vedaMemPtr((void **)&gradInData, vGradInData);

  vednnError_t res = vednnConvolutionBackwardData(paramGradOut, gradOutData, paramFilter, weightData, paramGradIn,
                                                  gradInData, paramConv, algo);

  if (res != VEDNN_SUCCESS) return res;

  // paramGradIn could be used for "in"
  // paramFilter could be used for "gradWeights"
  res = vednnConvolutionBackwardFilter(paramGradIn, inData, paramGradOut, gradOutData, paramFilter, gradWeightsData,
                                       paramConv, algo);

  return res;
}

vednnError_t vedaVednnActivationForward(const vednnActivationMode_t mode, VEDAdeviceptr vDataIn, VEDAdeviceptr vDataOut,
                                        const unsigned long nElements) {
  LOG_FUNC();
  void *pDataIn;
  void *pDataOut;
  vedaMemPtr((void **)&pDataIn, vDataIn);
  vedaMemPtr((void **)&pDataOut, vDataOut);

  return vednnActivationForward(mode, pDataIn, pDataOut, nElements);
}

vednnError_t vedaVednnActivationBackward(const vednnActivationMode_t mode, VEDAdeviceptr vDataGradOut,
                                         VEDAdeviceptr vDataIn, VEDAdeviceptr vDataGradIn,
                                         const unsigned long nElements) {
  LOG_FUNC();
  void *pDataGradOut;
  void *pDataIn;
  void *pDataGradIn;
  vedaMemPtr((void **)&pDataGradOut, vDataGradOut);
  vedaMemPtr((void **)&pDataIn, vDataIn);
  vedaMemPtr((void **)&pDataGradIn, vDataGradIn);

  return vednnActivationBackward(mode, pDataGradOut, pDataIn, pDataGradIn, nElements);
}

vednnError_t vedaVednnSoftmaxForward(const vednnSoftmaxMode_t mode, VEDAdeviceptr vDataIn, VEDAdeviceptr vDataOut,
                                     const unsigned long nBatch, const unsigned long nClass) {
  LOG_FUNC();
  void *pDataIn;
  void *pDataOut;
  vedaMemPtr((void **)&pDataIn, vDataIn);
  vedaMemPtr((void **)&pDataOut, vDataOut);

  return vednnSoftmaxForward(mode, pDataIn, pDataOut, nBatch, nClass);
}

vednnError_t vedaVednnLinearForwardExF32(unsigned long bGemm, const unsigned long inDim, const unsigned long outDim,
                                         const unsigned long nBatch, VEDAdeviceptr vX, const unsigned long xStride,
                                         VEDAdeviceptr vY, const unsigned long yStride, VEDAdeviceptr vZ,
                                         const unsigned long zStride) {
  LOG_FUNC();
  vednnError_t res;
  float *x, *y;
  float *z;
  vedaMemPtr((void **)&x, vX);
  vedaMemPtr((void **)&y, vY);
  vedaMemPtr((void **)&z, vZ);

  if (bGemm == 1) {
    return vednnLinearForward(inDim, outDim, nBatch, 1, x, y, z);
  } else {
    // because of the bgemm did not work as expected, we will manually parallelize over bGemm

    //#pragma omp parallel for
    for (int i = 0; i < bGemm; i++) {
      float *xPtr = x + i * xStride;
      float *yPtr = y + i * yStride;
      float *zPtr = z + i * zStride;
      vednnLinearForward(inDim, outDim, nBatch, 1, xPtr, yPtr, zPtr);
    }
    // WARNING: we will silently return success
    return VEDNN_SUCCESS;
  }
}

vednnError_t vedaVednnMaxPoolingForward(const vednnTensorParam_t *pParamIn, VEDAdeviceptr vDataIn,
                                        const vednnTensorParam_t *pParamOut, VEDAdeviceptr vDataOut,
                                        const vednnPoolingParam_t *pParamPool) {
  LOG_FUNC();
  void *pDataIn;
  void *pDataOut;
  vedaMemPtr((void **)&pDataIn, vDataIn);
  vedaMemPtr((void **)&pDataOut, vDataOut);
  return vednnMaxPoolingForward(pParamIn, pDataIn, pParamOut, pDataOut, pParamPool);
}

vednnError_t vedaVednnMaxPoolingBackwardEx(const vednnTensorParam_t *pParamGradOut, VEDAdeviceptr vDataGradOut,
                                           const vednnTensorParam_t *pParamOut, VEDAdeviceptr vDataOut,
                                           const vednnTensorParam_t *pParamIn, VEDAdeviceptr vDataIn,
                                           const vednnTensorParam_t *pParamGradIn, VEDAdeviceptr vDataGradIn,
                                           const vednnPoolingParam_t *pParamPool) {
  LOG_FUNC();
  void *pDataGradOut, *pDataIn, *pDataGradIn, *pDataOut;
  vedaMemPtr((void **)&pDataGradOut, vDataGradOut);
  vedaMemPtr((void **)&pDataIn, vDataIn);
  vedaMemPtr((void **)&pDataOut, vDataOut);
  vedaMemPtr((void **)&pDataGradIn, vDataGradIn);

  vednnError_t res = vednnMaxPoolingForward(pParamIn, pDataIn, pParamOut, pDataOut, pParamPool);

  if (res == VEDNN_SUCCESS) {
    vednnMaxPoolingBackward(pParamGradOut, pDataGradOut, pParamOut, pDataOut, pParamIn, pDataIn, pParamGradIn,
                            pDataGradIn, pParamPool);
  }
  return res;
}
}
