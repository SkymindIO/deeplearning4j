<html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">.lst-kix_1owp9c5rf9na-7>li:before{content:"\0025cb  "}.lst-kix_1owp9c5rf9na-6>li:before{content:"\0025cf  "}.lst-kix_1owp9c5rf9na-8>li:before{content:"\0025a0  "}ul.lst-kix_1owp9c5rf9na-0{list-style-type:none}ul.lst-kix_1owp9c5rf9na-1{list-style-type:none}ul.lst-kix_1owp9c5rf9na-2{list-style-type:none}.lst-kix_9mdf3bddnkmc-0>li:before{content:"\0025cf  "}ul.lst-kix_1owp9c5rf9na-3{list-style-type:none}ul.lst-kix_1owp9c5rf9na-4{list-style-type:none}ul.lst-kix_1owp9c5rf9na-5{list-style-type:none}.lst-kix_1owp9c5rf9na-4>li:before{content:"\0025cb  "}ul.lst-kix_1owp9c5rf9na-6{list-style-type:none}ul.lst-kix_1owp9c5rf9na-7{list-style-type:none}ul.lst-kix_1owp9c5rf9na-8{list-style-type:none}.lst-kix_1owp9c5rf9na-5>li:before{content:"\0025a0  "}.lst-kix_1owp9c5rf9na-0>li:before{content:"\0025cf  "}.lst-kix_9mdf3bddnkmc-5>li:before{content:"\0025a0  "}.lst-kix_9mdf3bddnkmc-6>li:before{content:"\0025cf  "}.lst-kix_9mdf3bddnkmc-4>li:before{content:"\0025cb  "}.lst-kix_1owp9c5rf9na-3>li:before{content:"\0025cf  "}.lst-kix_9mdf3bddnkmc-1>li:before{content:"\0025cb  "}.lst-kix_9mdf3bddnkmc-2>li:before{content:"\0025a0  "}.lst-kix_1owp9c5rf9na-1>li:before{content:"\0025cb  "}.lst-kix_1owp9c5rf9na-2>li:before{content:"\0025a0  "}.lst-kix_9mdf3bddnkmc-3>li:before{content:"\0025cf  "}ul.lst-kix_bz51fvebkjbm-0{list-style-type:none}ul.lst-kix_bz51fvebkjbm-1{list-style-type:none}ul.lst-kix_9mdf3bddnkmc-0{list-style-type:none}ul.lst-kix_bz51fvebkjbm-2{list-style-type:none}ul.lst-kix_bz51fvebkjbm-3{list-style-type:none}ul.lst-kix_bz51fvebkjbm-4{list-style-type:none}ul.lst-kix_bz51fvebkjbm-5{list-style-type:none}ul.lst-kix_9mdf3bddnkmc-5{list-style-type:none}.lst-kix_1ncblnoedfvh-7>li:before{content:"\0025cb  "}ul.lst-kix_9mdf3bddnkmc-6{list-style-type:none}ul.lst-kix_9mdf3bddnkmc-7{list-style-type:none}ul.lst-kix_9mdf3bddnkmc-8{list-style-type:none}ul.lst-kix_9mdf3bddnkmc-1{list-style-type:none}.lst-kix_1ncblnoedfvh-5>li:before{content:"\0025a0  "}.lst-kix_1ncblnoedfvh-6>li:before{content:"\0025cf  "}ul.lst-kix_9mdf3bddnkmc-2{list-style-type:none}ul.lst-kix_9mdf3bddnkmc-3{list-style-type:none}ul.lst-kix_9mdf3bddnkmc-4{list-style-type:none}.lst-kix_1ncblnoedfvh-3>li:before{content:"\0025cf  "}.lst-kix_1ncblnoedfvh-4>li:before{content:"\0025cb  "}ul.lst-kix_1ncblnoedfvh-0{list-style-type:none}ul.lst-kix_bz51fvebkjbm-6{list-style-type:none}ul.lst-kix_1ncblnoedfvh-5{list-style-type:none}ul.lst-kix_bz51fvebkjbm-7{list-style-type:none}.lst-kix_1ncblnoedfvh-0>li:before{content:"\0025cf  "}ul.lst-kix_1ncblnoedfvh-6{list-style-type:none}ul.lst-kix_bz51fvebkjbm-8{list-style-type:none}ul.lst-kix_1ncblnoedfvh-7{list-style-type:none}ul.lst-kix_1ncblnoedfvh-8{list-style-type:none}ul.lst-kix_1ncblnoedfvh-1{list-style-type:none}.lst-kix_1ncblnoedfvh-1>li:before{content:"\0025cb  "}.lst-kix_1ncblnoedfvh-2>li:before{content:"\0025a0  "}ul.lst-kix_1ncblnoedfvh-2{list-style-type:none}ul.lst-kix_1ncblnoedfvh-3{list-style-type:none}.lst-kix_bz51fvebkjbm-4>li:before{content:"\0025cb  "}ul.lst-kix_1ncblnoedfvh-4{list-style-type:none}.lst-kix_uu2uwssfvz8x-0>li:before{content:"\0025cf  "}.lst-kix_bz51fvebkjbm-3>li:before{content:"\0025cf  "}.lst-kix_bz51fvebkjbm-5>li:before{content:"\0025a0  "}.lst-kix_uu2uwssfvz8x-2>li:before{content:"\0025a0  "}.lst-kix_bz51fvebkjbm-2>li:before{content:"\0025a0  "}.lst-kix_bz51fvebkjbm-6>li:before{content:"\0025cf  "}.lst-kix_bz51fvebkjbm-1>li:before{content:"\0025cb  "}.lst-kix_bz51fvebkjbm-7>li:before{content:"\0025cb  "}.lst-kix_uu2uwssfvz8x-1>li:before{content:"\0025cb  "}.lst-kix_bz51fvebkjbm-0>li:before{content:"\0025cf  "}.lst-kix_bz51fvebkjbm-8>li:before{content:"\0025a0  "}ul.lst-kix_uu2uwssfvz8x-4{list-style-type:none}ul.lst-kix_uu2uwssfvz8x-3{list-style-type:none}.lst-kix_uu2uwssfvz8x-7>li:before{content:"\0025cb  "}.lst-kix_uu2uwssfvz8x-8>li:before{content:"\0025a0  "}ul.lst-kix_uu2uwssfvz8x-6{list-style-type:none}ul.lst-kix_uu2uwssfvz8x-5{list-style-type:none}ul.lst-kix_uu2uwssfvz8x-8{list-style-type:none}.lst-kix_uu2uwssfvz8x-6>li:before{content:"\0025cf  "}ul.lst-kix_uu2uwssfvz8x-7{list-style-type:none}.lst-kix_1ncblnoedfvh-8>li:before{content:"\0025a0  "}.lst-kix_uu2uwssfvz8x-3>li:before{content:"\0025cf  "}.lst-kix_uu2uwssfvz8x-4>li:before{content:"\0025cb  "}ul.lst-kix_uu2uwssfvz8x-0{list-style-type:none}.lst-kix_uu2uwssfvz8x-5>li:before{content:"\0025a0  "}ul.lst-kix_uu2uwssfvz8x-2{list-style-type:none}ul.lst-kix_uu2uwssfvz8x-1{list-style-type:none}.lst-kix_9mdf3bddnkmc-7>li:before{content:"\0025cb  "}.lst-kix_9mdf3bddnkmc-8>li:before{content:"\0025a0  "}ol{margin:0;padding:0}table td,table th{padding:0}.c3{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:1.5pt;border-right-width:1.5pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1.5pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1.5pt;width:54.2pt;border-top-color:#000000;border-bottom-style:solid}.c16{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:1.5pt;border-right-width:1.5pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1.5pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1.5pt;width:49pt;border-top-color:#000000;border-bottom-style:solid}.c15{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:1.5pt;border-right-width:1.5pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1.5pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1.5pt;width:179.5pt;border-top-color:#000000;border-bottom-style:solid}.c8{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:1.5pt;border-right-width:1.5pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1.5pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1.5pt;width:103pt;border-top-color:#000000;border-bottom-style:solid}.c22{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:1.5pt;border-right-width:1.5pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1.5pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1.5pt;width:53.5pt;border-top-color:#000000;border-bottom-style:solid}.c6{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial";font-style:normal}.c0{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:right}.c14{padding-top:0pt;padding-bottom:0pt;line-height:1.15;text-align:left}.c23{margin-left:13pt;padding-top:13pt;padding-bottom:13pt;margin-right:13pt}.c5{margin-left:72pt;orphans:2;widows:2;padding-left:0pt}.c9{border-spacing:0;border-collapse:collapse;margin-right:auto}.c25{padding-top:24pt;padding-bottom:5pt;text-align:center}.c1{padding-bottom:4pt;orphans:2;widows:2}.c18{font-size:13pt;color:#000000;font-weight:700}.c26{background-color:#ffffff;max-width:468pt;padding:72pt 72pt 72pt 72pt}.c11{orphans:2;widows:2;height:11pt}.c17{margin-left:36pt;padding-left:0pt}.c7{color:#1155cc;text-decoration:underline}.c12{orphans:2;widows:2}.c2{padding:0;margin:0}.c20{font-size:17pt;font-weight:700}.c29{font-size:23pt;font-weight:700}.c13{color:inherit;text-decoration:inherit}.c10{height:0pt}.c27{font-style:italic}.c28{padding-top:14pt}.c21{vertical-align:super}.c4{color:#5f8700}.c24{padding-bottom:11pt}.c19{color:#757575}.title{padding-top:0pt;color:#000000;font-size:26pt;padding-bottom:3pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.subtitle{padding-top:0pt;color:#666666;font-size:15pt;padding-bottom:16pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}li{color:#000000;font-size:11pt;font-family:"Arial"}p{margin:0;color:#000000;font-size:11pt;font-family:"Arial"}h1{padding-top:20pt;color:#000000;font-size:20pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h2{padding-top:18pt;color:#000000;font-size:16pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h3{padding-top:16pt;color:#434343;font-size:14pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h4{padding-top:14pt;color:#666666;font-size:12pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h5{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h6{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}</style></head><body class="c26"><h1 class="c12 c25" id="h.nlylmae8amzi"><span class="c29">DL4J &amp; MxNet : Contrasting 2 Frameworks</span></h1><h2 class="c1" id="h.4vrrkqezkazq"><span class="c20">Table of Contents</span></h2><ul class="c2 lst-kix_bz51fvebkjbm-0 start"><li class="c17 c12"><span>Activity on</span><span>&nbsp;Deeplearning4j</span></li><li class="c17 c12"><span>On interactivity and Notebooks</span></li><li class="c17 c12"><span>On compute performance vs. potential</span></li></ul><ul class="c2 lst-kix_bz51fvebkjbm-1 start"><li class="c5"><span>The potential for dealing with the multi-tenancy use case in an enterprise environment</span></li><li class="c5"><span>The potential for fault tolerance</span></li><li class="c5"><span>The potential for massive distribution gains</span></li></ul><h2 class="c1" id="h.nafi5jchf4j8"><span class="c20">Activity on Deeplearning4j</span></h2><p class="c12"><span>L</span><span>et&#39;s look at activity on the deeplearning4j pull requests in November 2016:</span></p><p class="c11"><span></span></p><a id="t.5a292c294b689b5b8e72e3addc9e0dc560f6c0c0"></a><a id="t.0"></a><table class="c9"><tbody><tr class="c10"><td class="c15" colspan="1" rowspan="1"><p class="c14"><span class="c6">repo_name</span></p></td><td class="c8" colspan="1" rowspan="1"><p class="c0"><span class="c6">PullRequestEvent</span></p></td><td class="c22" colspan="1" rowspan="1"><p class="c0"><span class="c6">authors</span></p></td><td class="c3" colspan="1" rowspan="1"><p class="c0"><span class="c6">merged</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c0"><span class="c6">closed</span></p></td></tr><tr class="c10"><td class="c15" colspan="1" rowspan="1"><p class="c12 c14"><span class="c6">deeplearning4j/deeplearning4j</span></p></td><td class="c8" colspan="1" rowspan="1"><p class="c0"><span class="c6">2429</span></p></td><td class="c22" colspan="1" rowspan="1"><p class="c0"><span class="c6">676</span></p></td><td class="c3" colspan="1" rowspan="1"><p class="c0"><span class="c6">62</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c0"><span class="c6">65</span></p></td></tr><tr class="c10"><td class="c15" colspan="1" rowspan="1"><p class="c14 c12"><span class="c6">deeplearning4j/dl4j-examples</span></p></td><td class="c8" colspan="1" rowspan="1"><p class="c0"><span class="c6">255</span></p></td><td class="c22" colspan="1" rowspan="1"><p class="c0"><span class="c6">125</span></p></td><td class="c3" colspan="1" rowspan="1"><p class="c0"><span class="c6">16</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c0"><span class="c6">12</span></p></td></tr><tr class="c10"><td class="c15" colspan="1" rowspan="1"><p class="c14 c12"><span class="c6">deeplearning4j/nd4j</span></p></td><td class="c8" colspan="1" rowspan="1"><p class="c0"><span class="c6">1371</span></p></td><td class="c22" colspan="1" rowspan="1"><p class="c0"><span class="c6">112</span></p></td><td class="c3" colspan="1" rowspan="1"><p class="c0"><span class="c6">60</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c0"><span class="c6">69</span></p></td></tr><tr class="c10"><td class="c15" colspan="1" rowspan="1"><p class="c14 c12"><span class="c6">deeplearning4j/DataVec</span></p></td><td class="c8" colspan="1" rowspan="1"><p class="c0"><span class="c6">291</span></p></td><td class="c22" colspan="1" rowspan="1"><p class="c0"><span class="c6">39</span></p></td><td class="c3" colspan="1" rowspan="1"><p class="c0"><span class="c6">17</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c0"><span class="c6">18</span></p></td></tr><tr class="c10"><td class="c15" colspan="1" rowspan="1"><p class="c14 c12"><span class="c6">deeplearning4j/rl4j</span></p></td><td class="c8" colspan="1" rowspan="1"><p class="c0"><span class="c6">37</span></p></td><td class="c22" colspan="1" rowspan="1"><p class="c0"><span class="c6">32</span></p></td><td class="c3" colspan="1" rowspan="1"><p class="c0"><span class="c6">1</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c0"><span class="c6">1</span></p></td></tr><tr class="c10"><td class="c15" colspan="1" rowspan="1"><p class="c14 c12"><span class="c6">deeplearning4j/ScalNet</span></p></td><td class="c8" colspan="1" rowspan="1"><p class="c0"><span class="c6">29</span></p></td><td class="c22" colspan="1" rowspan="1"><p class="c0"><span class="c6">26</span></p></td><td class="c3" colspan="1" rowspan="1"><p class="c0"><span class="c6">0</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c0"><span class="c6">0</span></p></td></tr><tr class="c10"><td class="c15" colspan="1" rowspan="1"><p class="c14 c12"><span class="c6">dmlc/mxnet</span></p></td><td class="c8" colspan="1" rowspan="1"><p class="c0"><span class="c6">169</span></p></td><td class="c22" colspan="1" rowspan="1"><p class="c0"><span class="c6">22</span></p></td><td class="c3" colspan="1" rowspan="1"><p class="c0"><span class="c6">133</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c0"><span class="c6">36</span></p></td></tr><tr class="c10"><td class="c15" colspan="1" rowspan="1"><p class="c14 c12"><span class="c6">deeplearning4j/libnd4j</span></p></td><td class="c8" colspan="1" rowspan="1"><p class="c0"><span class="c6">477</span></p></td><td class="c22" colspan="1" rowspan="1"><p class="c0"><span class="c6">19</span></p></td><td class="c3" colspan="1" rowspan="1"><p class="c0"><span class="c6">22</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c0"><span class="c6">27</span></p></td></tr><tr class="c10"><td class="c15" colspan="1" rowspan="1"><p class="c14 c12"><span class="c6">deeplearning4j/nd4s</span></p></td><td class="c8" colspan="1" rowspan="1"><p class="c0"><span class="c6">15</span></p></td><td class="c22" colspan="1" rowspan="1"><p class="c0"><span class="c6">11</span></p></td><td class="c3" colspan="1" rowspan="1"><p class="c0"><span class="c6">0</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c0"><span class="c6">0</span></p></td></tr><tr class="c10"><td class="c15" colspan="1" rowspan="1"><p class="c14 c12"><span class="c6">dmlc/xgboost</span></p></td><td class="c8" colspan="1" rowspan="1"><p class="c0"><span class="c6">41</span></p></td><td class="c22" colspan="1" rowspan="1"><p class="c0"><span class="c6">7</span></p></td><td class="c3" colspan="1" rowspan="1"><p class="c0"><span class="c6">28</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c0"><span class="c6">13</span></p></td></tr><tr class="c10"><td class="c15" colspan="1" rowspan="1"><p class="c14 c12"><span class="c6">deeplearning4j/Arbiter</span></p></td><td class="c8" colspan="1" rowspan="1"><p class="c0"><span class="c6">21</span></p></td><td class="c22" colspan="1" rowspan="1"><p class="c0"><span class="c6">5</span></p></td><td class="c3" colspan="1" rowspan="1"><p class="c0"><span class="c6">3</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c0"><span class="c6">3</span></p></td></tr><tr class="c10"><td class="c15" colspan="1" rowspan="1"><p class="c14 c12"><span class="c6">deeplearning4j/dl4j-test-resources</span></p></td><td class="c8" colspan="1" rowspan="1"><p class="c0"><span class="c6">6</span></p></td><td class="c22" colspan="1" rowspan="1"><p class="c0"><span class="c6">4</span></p></td><td class="c3" colspan="1" rowspan="1"><p class="c0"><span class="c6">1</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c0"><span class="c6">1</span></p></td></tr><tr class="c10"><td class="c15" colspan="1" rowspan="1"><p class="c14 c12"><span class="c6">dmlc/web-data</span></p></td><td class="c8" colspan="1" rowspan="1"><p class="c0"><span class="c6">6</span></p></td><td class="c22" colspan="1" rowspan="1"><p class="c0"><span class="c6">4</span></p></td><td class="c3" colspan="1" rowspan="1"><p class="c0"><span class="c6">5</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c0"><span class="c6">1</span></p></td></tr><tr class="c10"><td class="c15" colspan="1" rowspan="1"><p class="c14 c12"><span class="c6">deeplearning4j/docker</span></p></td><td class="c8" colspan="1" rowspan="1"><p class="c0"><span class="c6">8</span></p></td><td class="c22" colspan="1" rowspan="1"><p class="c0"><span class="c6">3</span></p></td><td class="c3" colspan="1" rowspan="1"><p class="c0"><span class="c6">0</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c0"><span class="c6">0</span></p></td></tr><tr class="c10"><td class="c15" colspan="1" rowspan="1"><p class="c14 c12"><span class="c6">dmlc/mshadow</span></p></td><td class="c8" colspan="1" rowspan="1"><p class="c0"><span class="c6">6</span></p></td><td class="c22" colspan="1" rowspan="1"><p class="c0"><span class="c6">3</span></p></td><td class="c3" colspan="1" rowspan="1"><p class="c0"><span class="c6">6</span></p></td><td class="c16" colspan="1" rowspan="1"><p class="c0"><span class="c6">0</span></p></td></tr></tbody></table><p class="c11"><span></span></p><p class="c12"><span>Deeplearning4J uses a bot to comment on its pull requests for continuous integration, so that the number of events does not reflect human action. But the other columns show activity comparable to or superior to mxnet. In sum, in mxnet, it&#39;s probably true more academics port models. With Deeplearning4j, users interact, use the library, report issues and review more. Other months reveal similar patterns as November.</span></p><p class="c11"><span></span></p><p class="c12"><span>Source, in </span><span class="c7"><a class="c13" href="https://www.google.com/url?q=https://cloud.google.com/bigquery/public-data/github&amp;sa=D&amp;ust=1483665624778000&amp;usg=AFQjCNG_zGzz-AY_r7PWJfZDxq2Nhz1iEA">BigQuery</a></span><span>:</span></p><p class="c12 c23"><span class="c4">SELECT</span><span><br> &nbsp;repo.</span><span class="c4">name</span><span>,<br> &nbsp;</span><span class="c4">COUNT</span><span>(*) PullRequestEvent,<br> &nbsp;</span><span class="c4">COUNT</span><span>(</span><span class="c4">DISTINCT</span><span>&nbsp;actor.id) authors,<br> &nbsp;</span><span class="c4">SUM</span><span>(</span><span class="c4">CASE</span><span>&nbsp;</span><span class="c4">WHEN</span><span>&nbsp;JSON_EXTRACT(payload, </span><span class="c19">&#39;$.pull_request.merged&#39;</span><span>) </span><span class="c4">IN</span><span>&nbsp;(</span><span class="c19">&#39;true&#39;</span><span>) </span><span class="c4">THEN</span><span>&nbsp;1 </span><span class="c4">ELSE</span><span>&nbsp;0 </span><span class="c4">END</span><span>) </span><span class="c4">AS</span><span>&nbsp;merged,<br> &nbsp;</span><span class="c4">SUM</span><span>(</span><span class="c4">CASE</span><span>&nbsp;</span><span class="c4">WHEN</span><span>&nbsp;JSON_EXTRACT(payload, </span><span class="c19">&#39;$.pull_request.merged&#39;</span><span>) </span><span class="c4">IN</span><span>&nbsp;(</span><span class="c19">&#39;false&#39;</span><span>) </span><span class="c4">THEN</span><span>&nbsp;1 </span><span class="c4">ELSE</span><span>&nbsp;0 </span><span class="c4">END</span><span>) </span><span class="c4">AS</span><span>&nbsp;closed,<br></span><span class="c4">FROM</span><span><br> &nbsp;[githubarchive:</span><span class="c4">month</span><span>.201611]<br></span><span class="c4">WHERE</span><span><br> &nbsp;</span><span class="c4">type</span><span>&nbsp;</span><span class="c4">IN</span><span>&nbsp;( </span><span class="c19">&#39;PullRequestEvent&#39;</span><span>)<br> &nbsp;</span><span class="c4">AND</span><span>&nbsp;JSON_EXTRACT(payload, </span><span class="c19">&#39;$.action&#39;</span><span>) </span><span class="c4">IN</span><span>&nbsp;(</span><span class="c19">&#39;&quot;closed&quot;&#39;</span><span>)<br> &nbsp;</span><span class="c4">AND</span><span>&nbsp;(repo.</span><span class="c4">name</span><span>&nbsp;</span><span class="c4">LIKE</span><span>&nbsp;&quot;dmlc/%&quot;) </span><span class="c4">OR</span><span>&nbsp;(repo.</span><span class="c4">name</span><span>&nbsp;</span><span class="c4">LIKE</span><span>&nbsp;&quot;deeplearning4j/%&quot;)<br></span><span class="c4">GROUP</span><span>&nbsp;</span><span class="c4">BY</span><span><br> &nbsp;repo.</span><span class="c4">name</span><span><br></span><span class="c4">ORDER</span><span>&nbsp;</span><span class="c4">BY</span><span><br> &nbsp;authors </span><span class="c4">DESC</span><span><br></span><span class="c4">LIMIT</span><span><br> &nbsp;15<br></span></p><p class="c12"><span>If model building is an issue, Deeplearning4J is working on model import (it has released Keras import already). In the long term, model description standards (e.g. </span><span class="c7"><a class="c13" href="https://www.google.com/url?q=http://dmg.org/pfa/index.html&amp;sa=D&amp;ust=1483665624791000&amp;usg=AFQjCNEx5CRha1_u1R8Oflpf9CLkBfH1WA">PFA</a></span><span>) may make this point moot.</span></p><p class="c11"><span></span></p><h2 class="c1" id="h.jr8afvvu46z9"><span class="c20">On interactivity and Notebooks</span></h2><p class="c12"><span>Scala is a language that is </span><span class="c7"><a class="c13" href="https://www.google.com/url?q=http://www.artima.com/pins1ed/combining-scala-and-java.html&amp;sa=D&amp;ust=1483665624793000&amp;usg=AFQjCNGZLSpTdtEv39ZDfYjZDUW2xSk85g">natively interoperable with Java at the source level</a></span><span>. This means that it is directly possible to call Java routines from Scala, with the same arguments, the same types, and no translation whatsoever between the two languages since they use the same bytecode at runtime. As a consequence, an interactive REPL and a notebook are readily available for the Java-based DeepLearning4J framework: those of Scala. It is hence completely possible to fire up a Scala REPL, with the DeepLearning4J JARs on classpath, and directly use DL4J model training. Or to do the same with a notebook.</span></p><p class="c11"><span></span></p><p class="c12"><span>In fact, this was demonstrated in a talk at ScalaDays this year:</span></p><p class="c12"><span class="c7"><a class="c13" href="https://www.google.com/url?q=https://www.youtube.com/watch?v%3DvaiE2yGdJSg&amp;sa=D&amp;ust=1483665624794000&amp;usg=AFQjCNHTjcAyibJFbEi1bqXYgh9laS4aCQ">https://www.youtube.com/watch?v=vaiE2yGdJSg</a></span></p><p class="c11"><span class="c7"><a class="c13" href="https://www.google.com/url?q=https://www.youtube.com/watch?v%3DvaiE2yGdJSg&amp;sa=D&amp;ust=1483665624794000&amp;usg=AFQjCNHTjcAyibJFbEi1bqXYgh9laS4aCQ"></a></span></p><p class="c12"><span>Moreover, DeepLearning4J, has had a friendly relationship with Scala for a while :</span></p><ul class="c2 lst-kix_uu2uwssfvz8x-0 start"><li class="c17 c12"><span>Adam Gibson gave a ScalaDays </span><span class="c7"><a class="c13" href="https://www.google.com/url?q=http://event.scaladays.org/scaladays-amsterdam-2015&amp;sa=D&amp;ust=1483665624795000&amp;usg=AFQjCNGC4V9ErFw2RwgQdQXuvrJV2F66Wg">keynote</a></span><span>&nbsp;in June 2015, and had a Scala API</span></li></ul><p class="c12"><span>at the time, while mxnet first released its Scala package in March 2016,</span></p><ul class="c2 lst-kix_1ncblnoedfvh-0 start"><li class="c17 c12"><span>it has a Keras-like Scala interoperability layer (</span><span class="c7"><a class="c13" href="https://www.google.com/url?q=https://github.com/deeplearning4j/ScalNet/&amp;sa=D&amp;ust=1483665624796000&amp;usg=AFQjCNFiYqGQmrjM8NUb386bVDIiML0Nqw">ScalNet</a></span><span>),</span></li><li class="c17 c12"><span>and Skymind is hiring, starting February 2017, a Scala veteran who worked with Martin Odersky on Scala&#39;s type system, and is a Spark-notebook committer.</span></li><li class="c17 c12"><span>Further developments on the Scala front-end are on its roadmap.</span></li></ul><p class="c11"><span></span></p><p class="c12"><span>In a word, the correct approach to interpret language choices for DeepLearning4J is not to see it as a Java-only framework. It is as a framework that admits Scala and Java as equal citizens for deep learning development, and in particular caters to the data scientists that require interactivity for experiments. But it does not ostracize the masses of talented Java programmers that are readily available at entreprises to productionize the training and re-training over time of cash-cow models in a multi-tenant, distributed context.</span></p><h2 class="c1" id="h.gr0sxo7q5axv"><span class="c20">On compute performance vs. potential</span></h2><p class="c12"><span>It is true that performance is not the main issue for choosing deep learning frameworks, at the moment, and as long as considered candidates are within earshot of each other.</span></p><p class="c12"><span>But in considering frameworks that are still relatively small and recent, it may be a better gamble to consider their potential rather than just their current state. And in looking at the potential of deep learning frameworks, three major concerns come to mind with the specific mindset of thinking of enterprise constraints: how do we deal with multi-tenancy, what&#39;s the story with fault tolerance, and where can there be massive gains in on-prem training ?</span></p><h3 class="c12 c28" id="h.2rq20arqouep"><span class="c18">The potential for dealing with the multi-tenancy use case in an enterprise environment</span></h3><p class="c12"><span>Multi-tenancy is an issue with any enterprise that wants to deal with on-prem training at scale: because machines are so expensive, especially when equipped with GPUs, they need to be shared among several teams.</span></p><p class="c11"><span></span></p><p class="c12"><span>The goal, here, is not just to sequence workloads on the cluster &mdash; for that, the old scientific computing grid managers of old would have been enough &mdash; it is to maximize utilization by running several workloads concurrently on the same hardware. Dealing with mixed workloads requires a complexity in resource accounting that is not present in the Python / C++world.</span></p><p class="c11"><span></span></p><p class="c12"><span>Now, mxnet has indeed </span><span class="c7"><a class="c13" href="https://www.google.com/url?q=https://github.com/dmlc/dmlc-core/blob/master/tracker/dmlc_tracker/yarn.py&amp;sa=D&amp;ust=1483665624800000&amp;usg=AFQjCNGzTzJpuks_oP0jSNgnQCc-DJh07w">a few scripts</a></span><span>&nbsp;that can make it run as a </span><span>YARN</span><span>&nbsp;task, but their deployment pattern is very different from DL4J &mdash; in particular in that they require everything in the brittle environment necessary for accelerated training to be preexisting on all machines (BLAS, CUDA, &hellip;). There is no DevOps support. DL4J has no such assumption, and deployment with Spark is for it a first-class mode of operation, where it piggybacks on the existing cluster managers to set up all necessary components.</span></p><p class="c11"><span></span></p><p class="c12"><span>But looking further than this, YARN is in itself limited in its ability to account for usage of high performance computation devices, namely GPUs (or coprocessors). It can only support GPU usage using tags, stating that a machine has indeed one (or several) devices, and constraining workers of a HPC task to be chosen among tagged machines. This crude mechanic is not sufficient (how do we deal with priority between GPU-requiring and non-GPU-requiring tasks ? they can all reserve a tagged machine), and does not bode well for multi-GPU settings (and yet mxnet does not currently even use tags: your machines had better all have GPUs).</span></p><p class="c11"><span></span></p><p class="c12"><span>On the other hand, container-based architectures have a clear mechanism of control on the usage of HPC devices, through </span><span class="c7"><a class="c13" href="https://www.google.com/url?q=http://www.slideshare.net/jpetazzo/cgroups-namespaces-and-beyond-what-are-containers-made-from-dockercon-europe-2015&amp;sa=D&amp;ust=1483665624802000&amp;usg=AFQjCNFMUHrTasOBo55ApfN06mkATdKBFQ">the devices cgroup</a></span><span>. The big data world is adopting containers </span><span class="c7"><a class="c13" href="https://www.google.com/url?q=https://www.youtube.com/watch?v%3D3rGJ0bt0UhE%26list%3DPLj6h78yzYM2PqgIGU1Qmi8nY7dqn9PCr4%26index%3D17&amp;sa=D&amp;ust=1483665624802000&amp;usg=AFQjCNEUdjW_CGduzwDflN3cMjCjVhXFww">at a breakneck pace</a></span><span>&nbsp;&mdash; and the JVM is centralizing on this way of provisioning and running applications at a </span><span class="c7"><a class="c13" href="https://www.google.com/url?q=http://www.zdnet.com/article/containers-likely-to-disrupt-java-virtual-machine-usage-survey-says/&amp;sa=D&amp;ust=1483665624803000&amp;usg=AFQjCNF6boikg7MoQauLVnEMI5eYlXCvqg">rapid clip</a></span><span>. It is therefore logical that container orchestration on a cluster is the next great frontier, and the big data world is racing to meet this need, with Kubernetes, Docker Swarm, and Mesos. And Mesos, as an example, is a scheduler that supports GPU accounting, in which the priorities for choosing which jobs reserve GPU-equipped machines are made clear through </span><span class="c7"><a class="c13" href="https://www.google.com/url?q=https://people.eecs.berkeley.edu/~alig/papers/mesos.pdf&amp;sa=D&amp;ust=1483665624804000&amp;usg=AFQjCNERyDsdUaDH_eFqpg_R8AepOcQ43Q">dominant resource fairness</a></span><span>, and for which GPU-requirements in tasks is a standard </span><span class="c7"><a class="c13" href="https://www.google.com/url?q=https://github.com/apache/spark/pull/14644&amp;sa=D&amp;ust=1483665624804000&amp;usg=AFQjCNGmjlIXLEVhY3zWfCpNGLqONZswrw">available in Spark 2.0</a></span><span>. Because this is done by the virtue of being container-aware and but a sliver of code, we can expect other container schedulers (Kubernetes, Swarm) to follow suit quickly.</span><span class="c21">1</span></p><p class="c11"><span></span></p><p class="c12"><span>In sum, by simply piggy-backing on Spark, in its three cluster modes (YARN, Mesos, </span><span class="c27">and standalone</span><span>), Deeplearning4J offers a pattern of deployment in an enterprise cluster that does not disturb existing workloads or increase the devops work. It is simply another workload among others, with the small incongruity of requiring GPUs in a regulated, accounted and transparent manner. mxnet could perhaps achieve the same level of integration, but this would be at the cost of a lot of code, and of spending a lot of time maintaining this integration as the best practices for running a big data cluster evolve fast.</span></p><h3 class="c12 c28" id="h.2pziiy8u0eeh"><span class="c18">The potential for fault tolerance</span></h3><p class="c12"><span>When investing in training at scale, it is a daily occurrence to deal with machine outages in a modern, large-scale cluster. For deep learning, where training times can reach weeks, it is even more problematic to deal with this issue.</span></p><p class="c11"><span></span></p><p class="c12"><span>Now, it is often argued that deep learning frameworks can deal with an occasional worker failure since the algorithms they are running are stochastic, and their convergence resists the infrequent loss of a random partial contributor to the computation. This is, on paper, true.</span></p><p class="c11"><span></span></p><p class="c12"><span>But it completely ignores that dealing with a failure goes beyond maintaining the mathematical convergence properties of the algorithm. It includes:</span></p><ul class="c2 lst-kix_9mdf3bddnkmc-0 start"><li class="c17 c12"><span>restarting dead executors (watchdog),</span></li><li class="c17 c12"><span>reintegrating them in the pool of executors,</span></li><li class="c12 c17"><span>scaling up and down the number of workers depending on the speed at which the job progresses through epochs (dynamic allocation),</span></li><li class="c17 c12"><span>launching a speculative version of a job when one of them is running disproportionately slow, hoping to bypass an issue local to the machine the straggler is running on (speculative execution)</span></li><li class="c17 c12"><span>dealing with the single points of failure in the architecture (hot-swappable parameter servers)</span></li><li class="c17 c12"><span>mitigating systematic failure patterns that can bring down many machines at once (e.g. </span><span class="c7"><a class="c13" href="https://www.google.com/url?q=https://issues.apache.org/jira/browse/HDFS-7489&amp;sa=D&amp;ust=1483665624810000&amp;usg=AFQjCNEYFBQQMjr7RDeLxPyEOQiR5bpFZA">HDFS-7489</a></span><span>)</span></li></ul><p class="c11"><span></span></p><p class="c12"><span>Every single one of these points is dealt with in a completely standard fashion in either Spark or one of the projects in its close ecosystem.</span></p><p class="c11"><span></span></p><p class="c12"><span>Moreover, DL4J has on its roadmap to explore backing up the model saved in their parameter server at regular intervals in Spark&#39;s memory, providing a mixed asynchronous-synchronous model that can offer a fault tolerance that is unprecedented in its ability to make long-running training on commodity clusters work.</span></p><h3 class="c12 c28" id="h.ifwfvcgml5c"><span class="c18">The potential for massive distribution gains</span></h3><p class="c12"><span>F</span><span>ive big players in the field of deep learning </span><span class="c27">have each backed a deep learning framework</span><span>&nbsp;: </span></p><p class="c12"><span>Google with TensorFlow, Facebook with Torch, Baidu with Paddle Paddle, Microsoft with CNTK, and AWS with mxnet&sup2;. It is expected that each of these companies will get a controlling share over the Project Management Committee of their OSS project, and all but one (Facebook) are backing a cloud offering</span><span>&sup3;</span><span>.</span></p><p class="c11"><span></span></p><p class="c12"><span>That&#39;s a lot of common points, to the point that the coincidence should strike and make one ponder. Is it possible that the open-source work done by researchers in developing models in any one of these frameworks is a </span><span class="c7"><a class="c13" href="https://www.google.com/url?q=https://en.wikipedia.org/wiki/Complementary_good&amp;sa=D&amp;ust=1483665624815000&amp;usg=AFQjCNEgVoB0I_muS_ryiV4iAjlOLd9OzQ">complementary good</a></span><span>&nbsp;(in the economic sense) to the cloud offering of the player in question?</span></p><p class="c11"><span></span></p><p class="c12"><span>For that, it would be necessary for those big players to ensure that when used for commercial purposes, the work of these researchers is trained on their commercial cloud offering. But isn&#39;t it nonetheless possible to run at scale, distributed on your own machines with these frameworks ?</span></p><p class="c11"><span></span></p><p class="c12"><span>The answer is that it is indeed possible to train on a distributed framework, but with a performance that puts the maturity of these frameworks to shame. Issues on Tensorflow&#39;s distributed performance show a </span><span class="c7"><a class="c13" href="https://www.google.com/url?q=https://github.com/tensorflow/tensorflow/issues/2397&amp;sa=D&amp;ust=1483665624817000&amp;usg=AFQjCNGrnR7BfrjFX0uSpATe15YAK0WccA">glimpse</a></span><span>&nbsp;of the notion. And yet three things are clear:</span></p><p class="c11"><span></span></p><ul class="c2 lst-kix_1owp9c5rf9na-0 start"><li class="c17 c12"><span>It is possible to make distributed training much faster given some good engineering effort. All these frameworks run a variant of </span><span class="c7"><a class="c13" href="https://www.google.com/url?q=https://www.cs.cmu.edu/~muli/file/ps.pdf&amp;sa=D&amp;ust=1483665624818000&amp;usg=AFQjCNHnzBnzD_KzPJtulIbg5TIZCMeT0Q">a parameter server</a></span><span>&nbsp;(DL4J &amp; mxnet included &mdash; mxnet runs </span><span class="c7"><a class="c13" href="https://www.google.com/url?q=https://github.com/dmlc/ps-lite&amp;sa=D&amp;ust=1483665624819000&amp;usg=AFQjCNEuKVuMxFo2mS0-YE9LjxBwlOzEgw">ps-lite</a></span><span>). This is basically running a distributed stochastic gradient descent (SGD), an algorithm from the 60&#39;s &mdash; then called ADALINE &mdash; that is inefficient when distributed because of a clearly wasteful AllReduce&#8308; communication pattern. References on how to improve on this </span><span class="c7"><a class="c13" href="https://www.google.com/url?q=https://people.eecs.berkeley.edu/~jfc/papers/14/Kylix.pdf&amp;sa=D&amp;ust=1483665624820000&amp;usg=AFQjCNHt9hhVKaMGum1csQp8x7NUYXgq8g">abound</a></span><span>, and </span><span class="c7"><a class="c13" href="https://www.google.com/url?q=https://github.com/NervanaSystems/neon/wiki/Parallelism-with-neon-on-Nervana-Cloud&amp;sa=D&amp;ust=1483665624820000&amp;usg=AFQjCNF7Qhhgho6-GwVG5ltLbB0CZH3jrw">have been known</a></span><span>&nbsp;by practitioners for a while.</span></li><li class="c17 c12"><span>Cloud players have a vested interest in making this open source distributed training inefficient for large-scale commercial applications. It drives commercial users to their (better) infrastructure (with specialized hardware). One hint of this is that when Microsoft became the only player to offer an optimized version of SGD in </span><span class="c7"><a class="c13" href="https://www.google.com/url?q=https://blogs.microsoft.com/next/2016/10/25/microsoft-releases-beta-microsoft-cognitive-toolkit-deep-learning-advances/&amp;sa=D&amp;ust=1483665624822000&amp;usg=AFQjCNEf9Cg4WBfJXIbGf5SoJna7LcGTKQ">CNTK 2.0 beta</a></span><span>, </span><span class="c7"><a class="c13" href="https://www.google.com/url?q=https://github.com/microsoft/cntk/wiki/CNTK-1bit-SGD-License&amp;sa=D&amp;ust=1483665624822000&amp;usg=AFQjCNHQzFC1M24cUUnsEyojFpsJzvwECQ">they specifically gave this optimized algorithm a non-commercial license</a></span><span>.</span></li><li class="c17 c12"><span>Deeplearning4J is the only framework working to make the distributed case fast, in an open-source </span><span class="c27">and free</span><span>&nbsp;manner. To wit, </span><span class="c7"><a class="c13" href="https://www.google.com/url?q=http://engineering.skymind.io/interview-with-adam-gibson-creator-of-dl4j-why-aeron-matters&amp;sa=D&amp;ust=1483665624824000&amp;usg=AFQjCNG3ryoSQCaUXSMM-xJCBk8F-7Sq6A">their rewrite of the DL4J parameter server with the blazingly fast Aeron library</a></span><span>. Here, the incentive is opposite for Skymind: the company has no cloud offering, and profits on supporting on-prem deployments.</span></li></ul><p class="c11"><span></span></p><p class="c12"><span>As a conclusion : if your focus is enterprise and especially on-prem, it would be in your interest to contribute to Deeplearning4j.</span></p><h2 class="c1" id="h.83p6k2auxv6j"><span class="c20">Footnotes:</span></h2><p class="c12 c24"><span class="c21">1</span><span>&nbsp;YARN can be made to run executors on containers. Yet its scheduling is not container-aware (here, containers just provide encapsulation) and has no known ambition to be in the future. The logic goes : containers track GPUs and container-aware schedulers can account for (and therefore maximize) their usage. YARN cannot aspire to the later.</span></p><p class="c12 c24"><span>&sup2; Note : AWS (the cloud division of Amazon), &nbsp;not Amazon Labs (the division which produced the now-defunct DSSTNE) : deep learning is </span><span class="c27">the business of the cloud</span><span>, now.</span></p><p class="c12 c24"><span>&sup3; Most recently Baidu: </span><span class="c7"><a class="c13" href="https://www.google.com/url?q=http://usa.chinadaily.com.cn/business/2016-12/01/content_27540310.htm&amp;sa=D&amp;ust=1483665624827000&amp;usg=AFQjCNEBky8yhRqCPXObXMY3jjy7CXpjyg">http://usa.chinadaily.com.cn/business/2016-12/01/content_27540310.htm</a></span></p><p class="c12 c24"><span>&#8308;</span><span>&nbsp;This is lingo from the MPI library.</span></p></body></html>

