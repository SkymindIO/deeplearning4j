<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<!-- NewPage -->
<html lang="en">
<head>
<!-- Generated by javadoc (1.8.0_111) on Thu Mar 30 11:58:05 PDT 2017 -->
<title>TransferLearning.Builder</title>
<meta name="date" content="2017-03-30">
<link rel="stylesheet" type="text/css" href="../../../../stylesheet.css" title="Style">
<script type="text/javascript" src="../../../../script.js"></script>
</head>
<body>
<script type="text/javascript"><!--
    try {
        if (location.href.indexOf('is-external=true') == -1) {
            parent.document.title="TransferLearning.Builder";
        }
    }
    catch(err) {
    }
//-->
var methods = {"i0":10,"i1":10,"i2":10,"i3":10,"i4":10,"i5":10,"i6":10,"i7":10,"i8":10,"i9":10,"i10":10,"i11":10,"i12":10};
var tabs = {65535:["t0","All Methods"],2:["t2","Instance Methods"],8:["t4","Concrete Methods"]};
var altColor = "altColor";
var rowColor = "rowColor";
var tableTab = "tableTab";
var activeTableTab = "activeTableTab";
</script>
<noscript>
<div>JavaScript is disabled on your browser.</div>
</noscript>
<!-- ========= START OF TOP NAVBAR ======= -->
<div class="topNav"><a name="navbar.top">
<!--   -->
</a>
<div class="skipNav"><a href="#skip.navbar.top" title="Skip navigation links">Skip navigation links</a></div>
<a name="navbar.top.firstrow">
<!--   -->
</a>
<ul class="navList" title="Navigation">
<li><a href="../../../../overview-summary.html">Overview</a></li>
<li><a href="package-summary.html">Package</a></li>
<li class="navBarCell1Rev">Class</li>
<li><a href="package-tree.html">Tree</a></li>
<li><a href="../../../../deprecated-list.html">Deprecated</a></li>
<li><a href="../../../../index-files/index-1.html">Index</a></li>
<li><a href="../../../../help-doc.html">Help</a></li>
</ul>
</div>
<div class="subNav">
<ul class="navList">
<li><a href="../../../../org/deeplearning4j/nn/transferlearning/TransferLearning.html" title="class in org.deeplearning4j.nn.transferlearning"><span class="typeNameLink">Prev&nbsp;Class</span></a></li>
<li><a href="../../../../org/deeplearning4j/nn/transferlearning/TransferLearning.GraphBuilder.html" title="class in org.deeplearning4j.nn.transferlearning"><span class="typeNameLink">Next&nbsp;Class</span></a></li>
</ul>
<ul class="navList">
<li><a href="../../../../index.html?org/deeplearning4j/nn/transferlearning/TransferLearning.Builder.html" target="_top">Frames</a></li>
<li><a href="TransferLearning.Builder.html" target="_top">No&nbsp;Frames</a></li>
</ul>
<ul class="navList" id="allclasses_navbar_top">
<li><a href="../../../../allclasses-noframe.html">All&nbsp;Classes</a></li>
</ul>
<div>
<script type="text/javascript"><!--
  allClassesLink = document.getElementById("allclasses_navbar_top");
  if(window==top) {
    allClassesLink.style.display = "block";
  }
  else {
    allClassesLink.style.display = "none";
  }
  //-->
</script>
</div>
<div>
<ul class="subNavList">
<li>Summary:&nbsp;</li>
<li>Nested&nbsp;|&nbsp;</li>
<li>Field&nbsp;|&nbsp;</li>
<li><a href="#constructor.summary">Constr</a>&nbsp;|&nbsp;</li>
<li><a href="#method.summary">Method</a></li>
</ul>
<ul class="subNavList">
<li>Detail:&nbsp;</li>
<li>Field&nbsp;|&nbsp;</li>
<li><a href="#constructor.detail">Constr</a>&nbsp;|&nbsp;</li>
<li><a href="#method.detail">Method</a></li>
</ul>
</div>
<a name="skip.navbar.top">
<!--   -->
</a></div>
<!-- ========= END OF TOP NAVBAR ========= -->
<!-- ======== START OF CLASS DATA ======== -->
<div class="header">
<div class="subTitle">org.deeplearning4j.nn.transferlearning</div>
<h2 title="Class TransferLearning.Builder" class="title">Class TransferLearning.Builder</h2>
</div>
<div class="contentContainer">
<ul class="inheritance">
<li>java.lang.Object</li>
<li>
<ul class="inheritance">
<li>org.deeplearning4j.nn.transferlearning.TransferLearning.Builder</li>
</ul>
</li>
</ul>
<div class="description">
<ul class="blockList">
<li class="blockList">
<dl>
<dt>Enclosing class:</dt>
<dd><a href="../../../../org/deeplearning4j/nn/transferlearning/TransferLearning.html" title="class in org.deeplearning4j.nn.transferlearning">TransferLearning</a></dd>
</dl>
<hr>
<br>
<pre>public static class <span class="typeNameLabel">TransferLearning.Builder</span>
extends java.lang.Object</pre>
</li>
</ul>
</div>
<div class="summary">
<ul class="blockList">
<li class="blockList">
<!-- ======== CONSTRUCTOR SUMMARY ======== -->
<ul class="blockList">
<li class="blockList"><a name="constructor.summary">
<!--   -->
</a>
<h3>Constructor Summary</h3>
<table class="memberSummary" border="0" cellpadding="3" cellspacing="0" summary="Constructor Summary table, listing constructors, and an explanation">
<caption><span>Constructors</span><span class="tabEnd">&nbsp;</span></caption>
<tr>
<th class="colOne" scope="col">Constructor and Description</th>
</tr>
<tr class="altColor">
<td class="colOne"><code><span class="memberNameLink"><a href="../../../../org/deeplearning4j/nn/transferlearning/TransferLearning.Builder.html#Builder-org.deeplearning4j.nn.multilayer.MultiLayerNetwork-">Builder</a></span>(<a href="../../../../org/deeplearning4j/nn/multilayer/MultiLayerNetwork.html" title="class in org.deeplearning4j.nn.multilayer">MultiLayerNetwork</a>&nbsp;origModel)</code>
<div class="block">Multilayer Network to tweak for transfer learning</div>
</td>
</tr>
</table>
</li>
</ul>
<!-- ========== METHOD SUMMARY =========== -->
<ul class="blockList">
<li class="blockList"><a name="method.summary">
<!--   -->
</a>
<h3>Method Summary</h3>
<table class="memberSummary" border="0" cellpadding="3" cellspacing="0" summary="Method Summary table, listing methods, and an explanation">
<caption><span id="t0" class="activeTableTab"><span>All Methods</span><span class="tabEnd">&nbsp;</span></span><span id="t2" class="tableTab"><span><a href="javascript:show(2);">Instance Methods</a></span><span class="tabEnd">&nbsp;</span></span><span id="t4" class="tableTab"><span><a href="javascript:show(8);">Concrete Methods</a></span><span class="tabEnd">&nbsp;</span></span></caption>
<tr>
<th class="colFirst" scope="col">Modifier and Type</th>
<th class="colLast" scope="col">Method and Description</th>
</tr>
<tr id="i0" class="altColor">
<td class="colFirst"><code><a href="../../../../org/deeplearning4j/nn/transferlearning/TransferLearning.Builder.html" title="class in org.deeplearning4j.nn.transferlearning">TransferLearning.Builder</a></code></td>
<td class="colLast"><code><span class="memberNameLink"><a href="../../../../org/deeplearning4j/nn/transferlearning/TransferLearning.Builder.html#addLayer-org.deeplearning4j.nn.conf.layers.Layer-">addLayer</a></span>(<a href="../../../../org/deeplearning4j/nn/conf/layers/Layer.html" title="class in org.deeplearning4j.nn.conf.layers">Layer</a>&nbsp;layer)</code>
<div class="block">Add layers to the net
 Required if layers are removed.</div>
</td>
</tr>
<tr id="i1" class="rowColor">
<td class="colFirst"><code><a href="../../../../org/deeplearning4j/nn/multilayer/MultiLayerNetwork.html" title="class in org.deeplearning4j.nn.multilayer">MultiLayerNetwork</a></code></td>
<td class="colLast"><code><span class="memberNameLink"><a href="../../../../org/deeplearning4j/nn/transferlearning/TransferLearning.Builder.html#build--">build</a></span>()</code>
<div class="block">Returns a model with the fine tune configuration and specified architecture changes.</div>
</td>
</tr>
<tr id="i2" class="altColor">
<td class="colFirst"><code><a href="../../../../org/deeplearning4j/nn/transferlearning/TransferLearning.Builder.html" title="class in org.deeplearning4j.nn.transferlearning">TransferLearning.Builder</a></code></td>
<td class="colLast"><code><span class="memberNameLink"><a href="../../../../org/deeplearning4j/nn/transferlearning/TransferLearning.Builder.html#fineTuneConfiguration-org.deeplearning4j.nn.transferlearning.FineTuneConfiguration-">fineTuneConfiguration</a></span>(<a href="../../../../org/deeplearning4j/nn/transferlearning/FineTuneConfiguration.html" title="class in org.deeplearning4j.nn.transferlearning">FineTuneConfiguration</a>&nbsp;finetuneConfiguration)</code>
<div class="block">Fine tune configurations specified will overwrite the existing configuration if any
 Usage example: specify a learning rate will set specified learning rate on all layers
 Refer to the fineTuneConfiguration class for more details</div>
</td>
</tr>
<tr id="i3" class="rowColor">
<td class="colFirst"><code><a href="../../../../org/deeplearning4j/nn/transferlearning/TransferLearning.Builder.html" title="class in org.deeplearning4j.nn.transferlearning">TransferLearning.Builder</a></code></td>
<td class="colLast"><code><span class="memberNameLink"><a href="../../../../org/deeplearning4j/nn/transferlearning/TransferLearning.Builder.html#nOutReplace-int-int-org.deeplearning4j.nn.conf.distribution.Distribution-">nOutReplace</a></span>(int&nbsp;layerNum,
           int&nbsp;nOut,
           <a href="../../../../org/deeplearning4j/nn/conf/distribution/Distribution.html" title="class in org.deeplearning4j.nn.conf.distribution">Distribution</a>&nbsp;dist)</code>
<div class="block">Modify the architecture of a layer by changing nOut
 Note this will also affect the layer that follows the layer specified, unless it is the output layer</div>
</td>
</tr>
<tr id="i4" class="altColor">
<td class="colFirst"><code><a href="../../../../org/deeplearning4j/nn/transferlearning/TransferLearning.Builder.html" title="class in org.deeplearning4j.nn.transferlearning">TransferLearning.Builder</a></code></td>
<td class="colLast"><code><span class="memberNameLink"><a href="../../../../org/deeplearning4j/nn/transferlearning/TransferLearning.Builder.html#nOutReplace-int-int-org.deeplearning4j.nn.conf.distribution.Distribution-org.deeplearning4j.nn.conf.distribution.Distribution-">nOutReplace</a></span>(int&nbsp;layerNum,
           int&nbsp;nOut,
           <a href="../../../../org/deeplearning4j/nn/conf/distribution/Distribution.html" title="class in org.deeplearning4j.nn.conf.distribution">Distribution</a>&nbsp;dist,
           <a href="../../../../org/deeplearning4j/nn/conf/distribution/Distribution.html" title="class in org.deeplearning4j.nn.conf.distribution">Distribution</a>&nbsp;distNext)</code>
<div class="block">Modify the architecture of a layer by changing nOut
 Note this will also affect the layer that follows the layer specified, unless it is the output layer
 Can specify different weight init schemes for the specified layer and the layer that follows it.</div>
</td>
</tr>
<tr id="i5" class="rowColor">
<td class="colFirst"><code><a href="../../../../org/deeplearning4j/nn/transferlearning/TransferLearning.Builder.html" title="class in org.deeplearning4j.nn.transferlearning">TransferLearning.Builder</a></code></td>
<td class="colLast"><code><span class="memberNameLink"><a href="../../../../org/deeplearning4j/nn/transferlearning/TransferLearning.Builder.html#nOutReplace-int-int-org.deeplearning4j.nn.conf.distribution.Distribution-org.deeplearning4j.nn.weights.WeightInit-">nOutReplace</a></span>(int&nbsp;layerNum,
           int&nbsp;nOut,
           <a href="../../../../org/deeplearning4j/nn/conf/distribution/Distribution.html" title="class in org.deeplearning4j.nn.conf.distribution">Distribution</a>&nbsp;dist,
           <a href="../../../../org/deeplearning4j/nn/weights/WeightInit.html" title="enum in org.deeplearning4j.nn.weights">WeightInit</a>&nbsp;schemeNext)</code>
<div class="block">Modify the architecture of a layer by changing nOut
 Note this will also affect the layer that follows the layer specified, unless it is the output layer
 Can specify different weight init schemes for the specified layer and the layer that follows it.</div>
</td>
</tr>
<tr id="i6" class="altColor">
<td class="colFirst"><code><a href="../../../../org/deeplearning4j/nn/transferlearning/TransferLearning.Builder.html" title="class in org.deeplearning4j.nn.transferlearning">TransferLearning.Builder</a></code></td>
<td class="colLast"><code><span class="memberNameLink"><a href="../../../../org/deeplearning4j/nn/transferlearning/TransferLearning.Builder.html#nOutReplace-int-int-org.deeplearning4j.nn.weights.WeightInit-">nOutReplace</a></span>(int&nbsp;layerNum,
           int&nbsp;nOut,
           <a href="../../../../org/deeplearning4j/nn/weights/WeightInit.html" title="enum in org.deeplearning4j.nn.weights">WeightInit</a>&nbsp;scheme)</code>
<div class="block">Modify the architecture of a layer by changing nOut
 Note this will also affect the layer that follows the layer specified, unless it is the output layer</div>
</td>
</tr>
<tr id="i7" class="rowColor">
<td class="colFirst"><code><a href="../../../../org/deeplearning4j/nn/transferlearning/TransferLearning.Builder.html" title="class in org.deeplearning4j.nn.transferlearning">TransferLearning.Builder</a></code></td>
<td class="colLast"><code><span class="memberNameLink"><a href="../../../../org/deeplearning4j/nn/transferlearning/TransferLearning.Builder.html#nOutReplace-int-int-org.deeplearning4j.nn.weights.WeightInit-org.deeplearning4j.nn.conf.distribution.Distribution-">nOutReplace</a></span>(int&nbsp;layerNum,
           int&nbsp;nOut,
           <a href="../../../../org/deeplearning4j/nn/weights/WeightInit.html" title="enum in org.deeplearning4j.nn.weights">WeightInit</a>&nbsp;scheme,
           <a href="../../../../org/deeplearning4j/nn/conf/distribution/Distribution.html" title="class in org.deeplearning4j.nn.conf.distribution">Distribution</a>&nbsp;distNext)</code>
<div class="block">Modify the architecture of a layer by changing nOut
 Note this will also affect the layer that follows the layer specified, unless it is the output layer
 Can specify different weight init schemes for the specified layer and the layer that follows it.</div>
</td>
</tr>
<tr id="i8" class="altColor">
<td class="colFirst"><code><a href="../../../../org/deeplearning4j/nn/transferlearning/TransferLearning.Builder.html" title="class in org.deeplearning4j.nn.transferlearning">TransferLearning.Builder</a></code></td>
<td class="colLast"><code><span class="memberNameLink"><a href="../../../../org/deeplearning4j/nn/transferlearning/TransferLearning.Builder.html#nOutReplace-int-int-org.deeplearning4j.nn.weights.WeightInit-org.deeplearning4j.nn.weights.WeightInit-">nOutReplace</a></span>(int&nbsp;layerNum,
           int&nbsp;nOut,
           <a href="../../../../org/deeplearning4j/nn/weights/WeightInit.html" title="enum in org.deeplearning4j.nn.weights">WeightInit</a>&nbsp;scheme,
           <a href="../../../../org/deeplearning4j/nn/weights/WeightInit.html" title="enum in org.deeplearning4j.nn.weights">WeightInit</a>&nbsp;schemeNext)</code>
<div class="block">Modify the architecture of a layer by changing nOut
 Note this will also affect the layer that follows the layer specified, unless it is the output layer
 Can specify different weight init schemes for the specified layer and the layer that follows it.</div>
</td>
</tr>
<tr id="i9" class="rowColor">
<td class="colFirst"><code><a href="../../../../org/deeplearning4j/nn/transferlearning/TransferLearning.Builder.html" title="class in org.deeplearning4j.nn.transferlearning">TransferLearning.Builder</a></code></td>
<td class="colLast"><code><span class="memberNameLink"><a href="../../../../org/deeplearning4j/nn/transferlearning/TransferLearning.Builder.html#removeLayersFromOutput-int-">removeLayersFromOutput</a></span>(int&nbsp;layerNum)</code>
<div class="block">Remove last "n" layers of the net
 At least an output layer must be added back in</div>
</td>
</tr>
<tr id="i10" class="altColor">
<td class="colFirst"><code><a href="../../../../org/deeplearning4j/nn/transferlearning/TransferLearning.Builder.html" title="class in org.deeplearning4j.nn.transferlearning">TransferLearning.Builder</a></code></td>
<td class="colLast"><code><span class="memberNameLink"><a href="../../../../org/deeplearning4j/nn/transferlearning/TransferLearning.Builder.html#removeOutputLayer--">removeOutputLayer</a></span>()</code>
<div class="block">Helper method to remove the outputLayer of the net.</div>
</td>
</tr>
<tr id="i11" class="rowColor">
<td class="colFirst"><code><a href="../../../../org/deeplearning4j/nn/transferlearning/TransferLearning.Builder.html" title="class in org.deeplearning4j.nn.transferlearning">TransferLearning.Builder</a></code></td>
<td class="colLast"><code><span class="memberNameLink"><a href="../../../../org/deeplearning4j/nn/transferlearning/TransferLearning.Builder.html#setFeatureExtractor-int-">setFeatureExtractor</a></span>(int&nbsp;layerNum)</code>
<div class="block">Specify a layer to set as a "feature extractor"
 The specified layer and the layers preceding it will be "frozen" with parameters staying constant</div>
</td>
</tr>
<tr id="i12" class="altColor">
<td class="colFirst"><code><a href="../../../../org/deeplearning4j/nn/transferlearning/TransferLearning.Builder.html" title="class in org.deeplearning4j.nn.transferlearning">TransferLearning.Builder</a></code></td>
<td class="colLast"><code><span class="memberNameLink"><a href="../../../../org/deeplearning4j/nn/transferlearning/TransferLearning.Builder.html#setInputPreProcessor-int-org.deeplearning4j.nn.conf.InputPreProcessor-">setInputPreProcessor</a></span>(int&nbsp;layer,
                    <a href="../../../../org/deeplearning4j/nn/conf/InputPreProcessor.html" title="interface in org.deeplearning4j.nn.conf">InputPreProcessor</a>&nbsp;processor)</code>
<div class="block">Specify the preprocessor for the added layers
 for cases where they cannot be inferred automatically.</div>
</td>
</tr>
</table>
<ul class="blockList">
<li class="blockList"><a name="methods.inherited.from.class.java.lang.Object">
<!--   -->
</a>
<h3>Methods inherited from class&nbsp;java.lang.Object</h3>
<code>clone, equals, finalize, getClass, hashCode, notify, notifyAll, toString, wait, wait, wait</code></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
<div class="details">
<ul class="blockList">
<li class="blockList">
<!-- ========= CONSTRUCTOR DETAIL ======== -->
<ul class="blockList">
<li class="blockList"><a name="constructor.detail">
<!--   -->
</a>
<h3>Constructor Detail</h3>
<a name="Builder-org.deeplearning4j.nn.multilayer.MultiLayerNetwork-">
<!--   -->
</a>
<ul class="blockListLast">
<li class="blockList">
<h4>Builder</h4>
<pre>public&nbsp;Builder(<a href="../../../../org/deeplearning4j/nn/multilayer/MultiLayerNetwork.html" title="class in org.deeplearning4j.nn.multilayer">MultiLayerNetwork</a>&nbsp;origModel)</pre>
<div class="block">Multilayer Network to tweak for transfer learning</div>
<dl>
<dt><span class="paramLabel">Parameters:</span></dt>
<dd><code>origModel</code> - </dd>
</dl>
</li>
</ul>
</li>
</ul>
<!-- ============ METHOD DETAIL ========== -->
<ul class="blockList">
<li class="blockList"><a name="method.detail">
<!--   -->
</a>
<h3>Method Detail</h3>
<a name="fineTuneConfiguration-org.deeplearning4j.nn.transferlearning.FineTuneConfiguration-">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>fineTuneConfiguration</h4>
<pre>public&nbsp;<a href="../../../../org/deeplearning4j/nn/transferlearning/TransferLearning.Builder.html" title="class in org.deeplearning4j.nn.transferlearning">TransferLearning.Builder</a>&nbsp;fineTuneConfiguration(<a href="../../../../org/deeplearning4j/nn/transferlearning/FineTuneConfiguration.html" title="class in org.deeplearning4j.nn.transferlearning">FineTuneConfiguration</a>&nbsp;finetuneConfiguration)</pre>
<div class="block">Fine tune configurations specified will overwrite the existing configuration if any
 Usage example: specify a learning rate will set specified learning rate on all layers
 Refer to the fineTuneConfiguration class for more details</div>
<dl>
<dt><span class="paramLabel">Parameters:</span></dt>
<dd><code>finetuneConfiguration</code> - </dd>
<dt><span class="returnLabel">Returns:</span></dt>
<dd>Builder</dd>
</dl>
</li>
</ul>
<a name="setFeatureExtractor-int-">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>setFeatureExtractor</h4>
<pre>public&nbsp;<a href="../../../../org/deeplearning4j/nn/transferlearning/TransferLearning.Builder.html" title="class in org.deeplearning4j.nn.transferlearning">TransferLearning.Builder</a>&nbsp;setFeatureExtractor(int&nbsp;layerNum)</pre>
<div class="block">Specify a layer to set as a "feature extractor"
 The specified layer and the layers preceding it will be "frozen" with parameters staying constant</div>
<dl>
<dt><span class="paramLabel">Parameters:</span></dt>
<dd><code>layerNum</code> - </dd>
<dt><span class="returnLabel">Returns:</span></dt>
<dd>Builder</dd>
</dl>
</li>
</ul>
<a name="nOutReplace-int-int-org.deeplearning4j.nn.weights.WeightInit-">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>nOutReplace</h4>
<pre>public&nbsp;<a href="../../../../org/deeplearning4j/nn/transferlearning/TransferLearning.Builder.html" title="class in org.deeplearning4j.nn.transferlearning">TransferLearning.Builder</a>&nbsp;nOutReplace(int&nbsp;layerNum,
                                            int&nbsp;nOut,
                                            <a href="../../../../org/deeplearning4j/nn/weights/WeightInit.html" title="enum in org.deeplearning4j.nn.weights">WeightInit</a>&nbsp;scheme)</pre>
<div class="block">Modify the architecture of a layer by changing nOut
 Note this will also affect the layer that follows the layer specified, unless it is the output layer</div>
<dl>
<dt><span class="paramLabel">Parameters:</span></dt>
<dd><code>layerNum</code> - The index of the layer to change nOut of</dd>
<dd><code>nOut</code> - Value of nOut to change to</dd>
<dd><code>scheme</code> - Weight Init scheme to use for params in layernum and layernum+1</dd>
<dt><span class="returnLabel">Returns:</span></dt>
<dd>Builder</dd>
</dl>
</li>
</ul>
<a name="nOutReplace-int-int-org.deeplearning4j.nn.conf.distribution.Distribution-">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>nOutReplace</h4>
<pre>public&nbsp;<a href="../../../../org/deeplearning4j/nn/transferlearning/TransferLearning.Builder.html" title="class in org.deeplearning4j.nn.transferlearning">TransferLearning.Builder</a>&nbsp;nOutReplace(int&nbsp;layerNum,
                                            int&nbsp;nOut,
                                            <a href="../../../../org/deeplearning4j/nn/conf/distribution/Distribution.html" title="class in org.deeplearning4j.nn.conf.distribution">Distribution</a>&nbsp;dist)</pre>
<div class="block">Modify the architecture of a layer by changing nOut
 Note this will also affect the layer that follows the layer specified, unless it is the output layer</div>
<dl>
<dt><span class="paramLabel">Parameters:</span></dt>
<dd><code>layerNum</code> - The index of the layer to change nOut of</dd>
<dd><code>nOut</code> - Value of nOut to change to</dd>
<dd><code>dist</code> - Distribution to use in conjunction with weight init DISTRIBUTION for params in layernum and layernum+1</dd>
<dt><span class="returnLabel">Returns:</span></dt>
<dd>Builder</dd>
<dt><span class="seeLabel">See Also:</span></dt>
<dd><a href="../../../../org/deeplearning4j/nn/weights/WeightInit.html" title="enum in org.deeplearning4j.nn.weights"><code>DISTRIBUTION</code></a></dd>
</dl>
</li>
</ul>
<a name="nOutReplace-int-int-org.deeplearning4j.nn.weights.WeightInit-org.deeplearning4j.nn.weights.WeightInit-">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>nOutReplace</h4>
<pre>public&nbsp;<a href="../../../../org/deeplearning4j/nn/transferlearning/TransferLearning.Builder.html" title="class in org.deeplearning4j.nn.transferlearning">TransferLearning.Builder</a>&nbsp;nOutReplace(int&nbsp;layerNum,
                                            int&nbsp;nOut,
                                            <a href="../../../../org/deeplearning4j/nn/weights/WeightInit.html" title="enum in org.deeplearning4j.nn.weights">WeightInit</a>&nbsp;scheme,
                                            <a href="../../../../org/deeplearning4j/nn/weights/WeightInit.html" title="enum in org.deeplearning4j.nn.weights">WeightInit</a>&nbsp;schemeNext)</pre>
<div class="block">Modify the architecture of a layer by changing nOut
 Note this will also affect the layer that follows the layer specified, unless it is the output layer
 Can specify different weight init schemes for the specified layer and the layer that follows it.</div>
<dl>
<dt><span class="paramLabel">Parameters:</span></dt>
<dd><code>layerNum</code> - The index of the layer to change nOut of</dd>
<dd><code>nOut</code> - Value of nOut to change to</dd>
<dd><code>scheme</code> - Weight Init scheme to use for params in the layerNum</dd>
<dd><code>schemeNext</code> - Weight Init scheme to use for params in the layerNum+1</dd>
<dt><span class="returnLabel">Returns:</span></dt>
<dd>Builder</dd>
</dl>
</li>
</ul>
<a name="nOutReplace-int-int-org.deeplearning4j.nn.conf.distribution.Distribution-org.deeplearning4j.nn.conf.distribution.Distribution-">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>nOutReplace</h4>
<pre>public&nbsp;<a href="../../../../org/deeplearning4j/nn/transferlearning/TransferLearning.Builder.html" title="class in org.deeplearning4j.nn.transferlearning">TransferLearning.Builder</a>&nbsp;nOutReplace(int&nbsp;layerNum,
                                            int&nbsp;nOut,
                                            <a href="../../../../org/deeplearning4j/nn/conf/distribution/Distribution.html" title="class in org.deeplearning4j.nn.conf.distribution">Distribution</a>&nbsp;dist,
                                            <a href="../../../../org/deeplearning4j/nn/conf/distribution/Distribution.html" title="class in org.deeplearning4j.nn.conf.distribution">Distribution</a>&nbsp;distNext)</pre>
<div class="block">Modify the architecture of a layer by changing nOut
 Note this will also affect the layer that follows the layer specified, unless it is the output layer
 Can specify different weight init schemes for the specified layer and the layer that follows it.</div>
<dl>
<dt><span class="paramLabel">Parameters:</span></dt>
<dd><code>layerNum</code> - The index of the layer to change nOut of</dd>
<dd><code>nOut</code> - Value of nOut to change to</dd>
<dd><code>dist</code> - Distribution to use for params in the layerNum</dd>
<dd><code>distNext</code> - Distribution to use for parmas in layerNum+1</dd>
<dt><span class="returnLabel">Returns:</span></dt>
<dd>Builder</dd>
<dt><span class="seeLabel">See Also:</span></dt>
<dd><a href="../../../../org/deeplearning4j/nn/weights/WeightInit.html" title="enum in org.deeplearning4j.nn.weights"><code>DISTRIBUTION</code></a></dd>
</dl>
</li>
</ul>
<a name="nOutReplace-int-int-org.deeplearning4j.nn.weights.WeightInit-org.deeplearning4j.nn.conf.distribution.Distribution-">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>nOutReplace</h4>
<pre>public&nbsp;<a href="../../../../org/deeplearning4j/nn/transferlearning/TransferLearning.Builder.html" title="class in org.deeplearning4j.nn.transferlearning">TransferLearning.Builder</a>&nbsp;nOutReplace(int&nbsp;layerNum,
                                            int&nbsp;nOut,
                                            <a href="../../../../org/deeplearning4j/nn/weights/WeightInit.html" title="enum in org.deeplearning4j.nn.weights">WeightInit</a>&nbsp;scheme,
                                            <a href="../../../../org/deeplearning4j/nn/conf/distribution/Distribution.html" title="class in org.deeplearning4j.nn.conf.distribution">Distribution</a>&nbsp;distNext)</pre>
<div class="block">Modify the architecture of a layer by changing nOut
 Note this will also affect the layer that follows the layer specified, unless it is the output layer
 Can specify different weight init schemes for the specified layer and the layer that follows it.</div>
<dl>
<dt><span class="paramLabel">Parameters:</span></dt>
<dd><code>layerNum</code> - The index of the layer to change nOut of</dd>
<dd><code>nOut</code> - Value of nOut to change to</dd>
<dd><code>scheme</code> - Weight init scheme to use for params in layerNum</dd>
<dd><code>distNext</code> - Distribution to use for parmas in layerNum+1</dd>
<dt><span class="returnLabel">Returns:</span></dt>
<dd>Builder</dd>
<dt><span class="seeLabel">See Also:</span></dt>
<dd><a href="../../../../org/deeplearning4j/nn/weights/WeightInit.html" title="enum in org.deeplearning4j.nn.weights"><code>DISTRIBUTION</code></a></dd>
</dl>
</li>
</ul>
<a name="nOutReplace-int-int-org.deeplearning4j.nn.conf.distribution.Distribution-org.deeplearning4j.nn.weights.WeightInit-">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>nOutReplace</h4>
<pre>public&nbsp;<a href="../../../../org/deeplearning4j/nn/transferlearning/TransferLearning.Builder.html" title="class in org.deeplearning4j.nn.transferlearning">TransferLearning.Builder</a>&nbsp;nOutReplace(int&nbsp;layerNum,
                                            int&nbsp;nOut,
                                            <a href="../../../../org/deeplearning4j/nn/conf/distribution/Distribution.html" title="class in org.deeplearning4j.nn.conf.distribution">Distribution</a>&nbsp;dist,
                                            <a href="../../../../org/deeplearning4j/nn/weights/WeightInit.html" title="enum in org.deeplearning4j.nn.weights">WeightInit</a>&nbsp;schemeNext)</pre>
<div class="block">Modify the architecture of a layer by changing nOut
 Note this will also affect the layer that follows the layer specified, unless it is the output layer
 Can specify different weight init schemes for the specified layer and the layer that follows it.</div>
<dl>
<dt><span class="paramLabel">Parameters:</span></dt>
<dd><code>layerNum</code> - The index of the layer to change nOut of</dd>
<dd><code>nOut</code> - Value of nOut to change to</dd>
<dd><code>dist</code> - Distribution to use for parmas in layerNum</dd>
<dd><code>schemeNext</code> - Weight init scheme to use for params in layerNum+1</dd>
<dt><span class="returnLabel">Returns:</span></dt>
<dd>Builder</dd>
<dt><span class="seeLabel">See Also:</span></dt>
<dd><a href="../../../../org/deeplearning4j/nn/weights/WeightInit.html" title="enum in org.deeplearning4j.nn.weights"><code>DISTRIBUTION</code></a></dd>
</dl>
</li>
</ul>
<a name="removeOutputLayer--">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>removeOutputLayer</h4>
<pre>public&nbsp;<a href="../../../../org/deeplearning4j/nn/transferlearning/TransferLearning.Builder.html" title="class in org.deeplearning4j.nn.transferlearning">TransferLearning.Builder</a>&nbsp;removeOutputLayer()</pre>
<div class="block">Helper method to remove the outputLayer of the net.
 Only one of the two - removeOutputLayer() or removeLayersFromOutput(layerNum) - can be specified
 When removing layers at the very least an output layer should be added with .addLayer(...)</div>
<dl>
<dt><span class="returnLabel">Returns:</span></dt>
<dd>Builder</dd>
</dl>
</li>
</ul>
<a name="removeLayersFromOutput-int-">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>removeLayersFromOutput</h4>
<pre>public&nbsp;<a href="../../../../org/deeplearning4j/nn/transferlearning/TransferLearning.Builder.html" title="class in org.deeplearning4j.nn.transferlearning">TransferLearning.Builder</a>&nbsp;removeLayersFromOutput(int&nbsp;layerNum)</pre>
<div class="block">Remove last "n" layers of the net
 At least an output layer must be added back in</div>
<dl>
<dt><span class="paramLabel">Parameters:</span></dt>
<dd><code>layerNum</code> - number of layers to remove</dd>
<dt><span class="returnLabel">Returns:</span></dt>
<dd>Builder</dd>
</dl>
</li>
</ul>
<a name="addLayer-org.deeplearning4j.nn.conf.layers.Layer-">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>addLayer</h4>
<pre>public&nbsp;<a href="../../../../org/deeplearning4j/nn/transferlearning/TransferLearning.Builder.html" title="class in org.deeplearning4j.nn.transferlearning">TransferLearning.Builder</a>&nbsp;addLayer(<a href="../../../../org/deeplearning4j/nn/conf/layers/Layer.html" title="class in org.deeplearning4j.nn.conf.layers">Layer</a>&nbsp;layer)</pre>
<div class="block">Add layers to the net
 Required if layers are removed. Can be called multiple times and layers will be added in the order with which they were called.
 At the very least an outputLayer must be added (output layer should be added last - as per the note on order)
 Learning configs (like updaters, learning rate etc) specified with the layer here will be honored</div>
<dl>
<dt><span class="paramLabel">Parameters:</span></dt>
<dd><code>layer</code> - layer conf to add (similar to the NeuralNetConfiguration .list().layer(...)</dd>
<dt><span class="returnLabel">Returns:</span></dt>
<dd>Builder</dd>
</dl>
</li>
</ul>
<a name="setInputPreProcessor-int-org.deeplearning4j.nn.conf.InputPreProcessor-">
<!--   -->
</a>
<ul class="blockList">
<li class="blockList">
<h4>setInputPreProcessor</h4>
<pre>public&nbsp;<a href="../../../../org/deeplearning4j/nn/transferlearning/TransferLearning.Builder.html" title="class in org.deeplearning4j.nn.transferlearning">TransferLearning.Builder</a>&nbsp;setInputPreProcessor(int&nbsp;layer,
                                                     <a href="../../../../org/deeplearning4j/nn/conf/InputPreProcessor.html" title="interface in org.deeplearning4j.nn.conf">InputPreProcessor</a>&nbsp;processor)</pre>
<div class="block">Specify the preprocessor for the added layers
 for cases where they cannot be inferred automatically.</div>
<dl>
<dt><span class="paramLabel">Parameters:</span></dt>
<dd><code>processor</code> - to be used on the data</dd>
<dt><span class="returnLabel">Returns:</span></dt>
<dd>Builder</dd>
</dl>
</li>
</ul>
<a name="build--">
<!--   -->
</a>
<ul class="blockListLast">
<li class="blockList">
<h4>build</h4>
<pre>public&nbsp;<a href="../../../../org/deeplearning4j/nn/multilayer/MultiLayerNetwork.html" title="class in org.deeplearning4j.nn.multilayer">MultiLayerNetwork</a>&nbsp;build()</pre>
<div class="block">Returns a model with the fine tune configuration and specified architecture changes.
 .init() need not be called. Can be directly fit.</div>
<dl>
<dt><span class="returnLabel">Returns:</span></dt>
<dd>MultiLayerNetwork</dd>
</dl>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
</div>
<!-- ========= END OF CLASS DATA ========= -->
<!-- ======= START OF BOTTOM NAVBAR ====== -->
<div class="bottomNav"><a name="navbar.bottom">
<!--   -->
</a>
<div class="skipNav"><a href="#skip.navbar.bottom" title="Skip navigation links">Skip navigation links</a></div>
<a name="navbar.bottom.firstrow">
<!--   -->
</a>
<ul class="navList" title="Navigation">
<li><a href="../../../../overview-summary.html">Overview</a></li>
<li><a href="package-summary.html">Package</a></li>
<li class="navBarCell1Rev">Class</li>
<li><a href="package-tree.html">Tree</a></li>
<li><a href="../../../../deprecated-list.html">Deprecated</a></li>
<li><a href="../../../../index-files/index-1.html">Index</a></li>
<li><a href="../../../../help-doc.html">Help</a></li>
</ul>
</div>
<div class="subNav">
<ul class="navList">
<li><a href="../../../../org/deeplearning4j/nn/transferlearning/TransferLearning.html" title="class in org.deeplearning4j.nn.transferlearning"><span class="typeNameLink">Prev&nbsp;Class</span></a></li>
<li><a href="../../../../org/deeplearning4j/nn/transferlearning/TransferLearning.GraphBuilder.html" title="class in org.deeplearning4j.nn.transferlearning"><span class="typeNameLink">Next&nbsp;Class</span></a></li>
</ul>
<ul class="navList">
<li><a href="../../../../index.html?org/deeplearning4j/nn/transferlearning/TransferLearning.Builder.html" target="_top">Frames</a></li>
<li><a href="TransferLearning.Builder.html" target="_top">No&nbsp;Frames</a></li>
</ul>
<ul class="navList" id="allclasses_navbar_bottom">
<li><a href="../../../../allclasses-noframe.html">All&nbsp;Classes</a></li>
</ul>
<div>
<script type="text/javascript"><!--
  allClassesLink = document.getElementById("allclasses_navbar_bottom");
  if(window==top) {
    allClassesLink.style.display = "block";
  }
  else {
    allClassesLink.style.display = "none";
  }
  //-->
</script>
</div>
<div>
<ul class="subNavList">
<li>Summary:&nbsp;</li>
<li>Nested&nbsp;|&nbsp;</li>
<li>Field&nbsp;|&nbsp;</li>
<li><a href="#constructor.summary">Constr</a>&nbsp;|&nbsp;</li>
<li><a href="#method.summary">Method</a></li>
</ul>
<ul class="subNavList">
<li>Detail:&nbsp;</li>
<li>Field&nbsp;|&nbsp;</li>
<li><a href="#constructor.detail">Constr</a>&nbsp;|&nbsp;</li>
<li><a href="#method.detail">Method</a></li>
</ul>
</div>
<a name="skip.navbar.bottom">
<!--   -->
</a></div>
<!-- ======== END OF BOTTOM NAVBAR ======= -->
</body>
</html>
