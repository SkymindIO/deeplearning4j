---
title: ニューラルネットワークのトレーニングにおけるトラブルシューティング
layout: default
---

# ニューラルネットワークのトレーニングにおけるトラブルシューティング

ニューラルネットワークは調節が難しいこともあります。ネットワークのハイパーパラメータを上手に選択しないと、学習速度が遅くなったり、全く学習しないことさえもあります。このページではネットワークの調節を行う際に押さえておくべき基本的手順をご紹介します。

これらのヒントの多くは、学術文献などでも既に論じられていますので、ここでは内容を1ページにまとめ、できるだけ明解に説明することを心がけました。

## 目次

* <a href="#normalization">データ正規化</a>
* <a href="#intro">重みの初期化</a>
* <a href="#epochs">エポック数とイテレーション数</a>
* <a href="#lrate">学習率</a>
* <a href="#activation">活性化関数</a>
* <a href="#loss">損失関数</a>
* <a href="#regularization">正規化</a>
* <a href="#minibatch">ミニバッチのサイズ</a>
* <a href="#updater">アップデーター及び最適化アルゴリズム</a>
* <a href="#normalization">データ正規化</a>
* <a href="#rnn">再帰型ニューラルネットワーク</a>
* <a href="#dbn">ディープ・ビリーフ・ネットワーク</a>
* <a href="#rbm">制限付きボルツマンマシン</a>
* <a href="#NaN">NaN（「Not a Number」非数）の問題</a>





## <a name="normalization">データ正規化</a>

データの分布はどうなっていますか？適切に量を測っていますか？一般に以下のようなことが言えます。

- 連続した値については、-1～1、0～1、または平均が0で標準偏差が1の正規分布が望ましいでしょう。ぴったり正確である必要はありませんが、入力データが大体この範囲内にあると、トレーニングをする際に助かります。入力データが膨大な場合は量を減らし、入力データが少ない場合は量を増やしてください。
- 非連続的なクラスについては（または出力データの分類に関する問題がある場合）、一般にはone-hot（ワン・ホット）表示を使います。つまり、クラスが3つある場合、データはそれぞれ[1,0,0]、[0,1,0]、[0,0,1]と表示されます。

トレーニングデータとテストデータには全く同じ正規化方法を使用することが重要であるということに注意してください。

## <a name="weight">重みの初期化</a>

Deeplearning4jは、weightInitパラメータを使って、いくつかの異なる種類の重み初期化をサポートしています。これらはweightInit(WeightInit)方法を使って設定されています。

重みが大きすぎないか小さすぎないかを確認してください。一般にこの作業にはXavier重み初期化が役に立ちます。正規化線形関数（relu）を使ったネットワークやリークのあるRELU重み初期化が賢明な選択でしょう。

## <a name="epochs">エポック数とイテレーション数</a>

エポック数はデータセットが完全に通過した回数を指します。DL4Jでは、イテレーションとは各ミニバッチにおいて、連続してパラメータが更新された回数を指します。

一般には、トレーニングの際、エポックの回数は複数回、イテレーションは一回（.iterations(1)を使う）にします。イテレーションを複数回行うのは、小規模のデータセットで全バッチのトレーニングを行う場合のみです。

エポックの回数が少なすぎると、ネットワークが良いパラメータを学習する時間が足りなくなり、多すぎるとトレーニングするデータが過剰適合してしまいます。エポック数を数える一つの方法は、early stoppingを適用することです。[Early stopping](http://deeplearning4j.org/earlystopping)を使うと、ニューラルネットワークの過剰適合を回避することができます（つまり、ネットワークがまだ見ていないデータを一般化するのに役に立ちます）。

## <a name="lrate">学習率</a>

学習率は複数あるハイパーパラメータの中でも一二を争うほど重要です。学習率が大きすぎたり小さすぎたりすると、ネットワークの学習状態が悪い、遅い、または全く学習しない、という結果になります。最も一般的な学習率の範囲は、 0.1～1e-6です。とはいえ、最適な学習率は、一般に個々のデータ（ネットワークのアーキテクチャー）によって異なります。お勧めする簡単な方法は、最初は1e-1、1e-3、1e-6の3つの異なる学習率を試すことにより大まかな値を検討した後で、細かい調節をするというものです。異なる学習率を同時に試すと時間が省けるのでお勧めです。

適切な学習率を選ぶためのアプローチ方法で一般的なものは、[DL4J's visualization interface（DL4Jの視覚化インターフェイス）](http://deeplearning4j.org/visualization)を使って、トレーニングの進捗状況を視覚化する、というものです。時間の経過とともに変化する損失、そして更新の大きさ対パラメータの大きさの比率（最初は約1:1000がいいでしょう）の両方に注目する必要があります。学習率の調節についての詳細は、[こちら](http://cs231n.github.io/neural-networks-3/#baby)をご覧ください。

分散された環境でニューラルネットワークをトレーニングするには、同じネットワークでも単一機械で行う場合とは異なる学習率（高めになることが多い）が必要になるかもしれません。

### ポリシーとスケジューリング

自分のニューラルネットワークに学習率のポリシーを設定することも可能です。ポリシーによって時間の経過とともに学習率を変更し、結果が向上します。これは、学習率は「速度を落として」収束する極小値を見つけることができるからです。一般に使われるポリシーはスケジューリングです。実際に使用される学習率のスケジュールは[LeNetのサンプル](https://github.com/deeplearning4j/dl4j-examples/blob/master/dl4j-examples/src/main/java/org/deeplearning4j/examples/convolution/LenetMnistExample.java)をご覧ください。

ただし、複数のGPUを使用されている場合は、スケジューリングにこれが影響しますのでご注意ください。例えば、GPUが2つある場合は、スケジュール内にあるイテレーションを2で割る必要があります。これは、トレーニングの過程で処理量が2倍になり、学習率のスケジュールはローカルのGPUでのみ適用可能だからです。

## <a name="activation">活性化関数</a>

活性化関数の選択において念頭に置くべきことが二つあります。

一つ目は、隠れ（出力用でない）層の活性化関数です。一般に、「relu」または「leakyrelu」の活性化を選ぶのがいいでしょう。その他の活性化関数（tanh、sigmoidなどだと、勾配消失の問題が発生しやすくなり、ディープ・ニューラル・ネットワークでの学習がより困難になります。しかし、長・短期記憶の層については、現在も一般にtanh活性化関数がよく使用されます。

二つ目は、出力層の活性化関数です。これは通常アプリケーションによります。分類の問題については、負の対数尤度／MCXENT（多クラスの交差エントロピー）と組み合わせたsoftmaxの活性化関数を使用するのがいいでしょう。- softmax活性化関数を使用すると、クラスの確率分布が得られます（出力の総和は1.0）。回帰の問題については、MSE（平均二乗誤差）損失関数とともに「恒等」関数を選ぶといいでしょう。

## <a name="loss">損失関数</a>

各ニューラルネットワーク層の損失関数は、事前トレーニングの際に適切な重みを把握したり、分類の際に（出力層での）結果を入手するために使用することができます。（上記の例では、分類はオーバーライドのセクションで発生します。）

ネットワークの目的によって、どの損失関数を使用するかが決まります。事前トレーニングには、再構成エントロピーを選びます。分類には、多クラスエントロピーを選びます。

## <a name="regularization">正規化</a>

正規化はトレーニング中における過剰適合の防止に役立ちます。過剰適合とは、あるモデルがトレーニングのデータには非常によく適合しても、実際に使用された時に過去に接したことのないデータに出くわすやいなや非常にパフォーマンスが悪くなったときに発生します。別の観点からいうと、過剰適合とはネットワークにトレーニングしたデータを記憶させるというものなのです（一般的な関係性を学習するというのではなく）。

一般的な正規化には、以下のものがあります。

- L１及びL2正規化は、ネットワークの大きな重みにペナルティーを課し、重みが大きくなりすぎるのを回避します。L2正規化のレベルのいくつかは実際によく使われます。しかし、L1やL2正規化の係数が大きすぎると、ネットワークにペナルティーを過剰に課してしまい、学習しなくなってしまいます。L2正規化の一般的な値は、1e-3～1e-6でとなっています。
- [ドロップアウト](./glossary.html#dropout)は正規化の方法としてよく使用されるもので、非常に効果があります。ドロップアウト率には、一般に0.5を使用します。
- ドロップコネクト（Dropconnect。概念はドロップアウトに似ていますが、あまり使用されていません。）
- ネットワークのサイズの全数に制限を設ける（つまり、層数や各層のサイズに限度を設ける）。
- [Early stopping](http://deeplearning4j.org/earlystopping)

L1／L2／ドロップアウト正規化を使用するには、regularization(true)の後に「l1(x), .l2(y), .dropout(z)」と続けてください。「dropout(z)」の「z」には活性化が保持される確率を入れます。

## <a name="minibatch">ミニバッチのサイズ</a>

ミニバッチとは、勾配及びパラメータの更新を計算しているときに、一回につき使用されるサンプル数を指します。実際は、データは一般にいくつかのミニバッチに分けます（かなり小規模のデータセットでない限り）。

最適なミニバッチのサイズは様々に異なります。例えば、ミニバッチのサイズが10だとGPU用には小さすぎますが、CPU用には問題ありません。また、ミニバッチのサイズが1だとネットワークのトレーニングは可能ですが、並列処理は実行されません。最初は32が妥当な数値でしょう。ミニバッチのサイズは一般的に16～128の範囲にあるからです（しかし、アプリケーションやネットワークの種類によって、この範囲を超えて少なくなったり多くなったりします）。

## <a name="updater">アップデーター及び最適化アルゴリズム</a>

DL4Jでは、「アップデーター」とは、モーメンタム、RMSProp、AdaGradなどのようなトレーニングのメカニズムを指します。これらの方法のどれかを使用すると、ありきたりな確率的勾配降下法と比べてネットワークのトレーニングのスピードがかなり増します。.updater(Updater)設定オプションを使ってアップデーターを設定することができます。

最適化アルゴリズムは、ある勾配においてどのように更新が行われるかに関するものです。最もシンプルな方法は、確率的勾配降下法ですが、DL4Jは確率的勾配降下法に他の最適化アルゴリズムである直線探索、共役勾配法、LBFGSも提供しています。これらのアルゴリズムは確率的勾配降下法よりも強力ですが、直線探索のコンポーネントが原因で、パラメータの更新一回につきよりコストが掛かります。このため、実際にはあまり使用されていません。原則としては、どのアップデーターをどの最適化アルゴリズムに組み合わせることも可能です。

ほとんどの場合、最初に選ぶといいのは、最適化アルゴリズムの確率的勾配降下法と共にアップグレーダーであるモーメンタム、RMSProp、Adagradのどれか一つと組み合わせることですが、モーメンタムをより頻繁に使用します。モーメンタムのアップデータはNESTEROVSと呼ばれており、慣性率は.momentum(double)オプションで設定することができます。

## <a name="resources">勾配正規化</a>

ニューラルネットワークをトレーニングしているときに、勾配が大きくなり過ぎないように、勾配正規化を行うのがいいことがあります（これは再帰型ニューラルネットワークによくある勾配爆発という問題です）。これは、.gradientNormalization(GradientNormalization)や.gradientNormalizationThreshould(double)の方法によって適用することができます。勾配正規化のサンプルは、[こちらの](https://github.com/deeplearning4j/deeplearning4j/blob/master/deeplearning4j-nn/src/main/java/org/deeplearning4j/nn/conf/GradientNormalization.java)「GradientNormalization.java」をご覧ください。このサンプルのテストコードは[こちら](https://github.com/deeplearning4j/deeplearning4j/blob/master/deeplearning4j-core/src/test/java/org/deeplearning4j/nn/updater/TestGradientNormalization.java)をご覧ください。

## <a name="rnn">再帰型ニューラルネットワーク：打ち切り型通時的逆伝播</a>

長い時系列を使って再帰型ニューラルネットワークをトレーニングするときは、一般に、打ち切り通時的逆伝播を使用するのがお勧めです。「標準的な」通時的逆伝播（DL4Jの初期設定）だと、1回のパラメータ更新でのコストはかなり大きくなる可能性があります。詳細については、[こちらのページ](http://deeplearning4j.org/usingrnns) と[こちらの](./glossary.html#backprop)用語集をご覧ください。

### <a name="ecosystem">可視／隠れユニット</a>

ディープ・ビリーフ・ニューラル・ネットワークを使用するときは、制限付きボルツマンマシン（特徴抽出に使うディープボルツマンマシンのコンポーネント）は確率的で、指定された可視ユニットまたは隠れユニットは異なる確率分布からサンプルを抽出します。

Geoff Hinton氏によって正確に書かれた[A Practical Guide to Training Restricted Boltzmann Machines（制限付きボルツマンマシンのトレーニングに関する解説書）](https://www.cs.toronto.edu/~hinton/absps/guideTR.pdf)をお読みいただくと、異なる確率分布のリストが記載されています。

## <a name="rbm">制限付きボルツマンマシン（RBM）</a>

圧縮を行う自己符号化器のために隠れ層を作成するときは、ニューロンは入力データより少なくしてください。隠れ層のノードが入力ノード数と近すぎると、恒等関数が再構築されてしまいます。隠れ層のニューロンが多すぎると、ノイズや過剰適合の増加を招きます。入力層数が784の場合は、最初の隠れ層数は500、2つ目の隠れ層数を250とするといいでしょう。隠れ層数が入力層のノード数の4分の1未満になることがないようにします。そして、出力層はラベル数と同じにするだけです。

データセットの規模が大きければその分隠れ層が多く必要になります。FacebookのDeep Faceは、莫大であることだけは確かなコーパスに9つの隠れ層を使用しています。多くの小規模なデータセットの場合、その分精度も落ちますが、隠れ層3つ、または4つのみで足りるかもしれません。一般に、大規模なデータセットの場合、より多くのバリエーションがあるため、正確な結果を得るためにより多くの特徴／ニューロンが必要になるのです。</p>典型的な機械学習では、もちろん、隠れ層は一つで、浅いネットワークはパーセプトロンと呼ばれています。

大規模なデータセットの場合、制限付きボルツマンマシンを何度か事前トレーニングしておく必要があります。何度か事前トレーニングして初めてアルゴリズムはデータセットのコンテクストで正確な重み特徴を学習するのです。とはいえ、事前トレーニングをスピードアップさせるために、並列処理やクラスターでデータを処理してもいいでしょう。

## <a name="">NaN（「Not a Number」非数）エラー</a>

Q.　ニューラルネットワークにNaN値が発生するのですが、どうしてですか？ 

A. バックプロパゲーションには非常に小さい勾配の乗法が含まれています。これは実数値を表示するときに適合率に限度があり、0に非常に近い値は表示できないからです。この問題は算術下位桁あふれ（Arithmetic Underflow）と呼ばれています。ニューラルネットワークにNaNが出現している場合、ネットワークを再調節して小さすぎる勾配を回避するのが解決策です。これは層が深めのニューラルネットワークに発生する問題です。 

倍精度小数点数型データ型を試してもいいですが、先にネットワークを再調節する方がいいでしょう。

調節を行うための基本的なヒントに従い、結果を監視するというのがNaNを二度と出現させない方法でしょう。



